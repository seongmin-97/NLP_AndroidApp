# 한국어 임베딩 - 01 임베딩이란

1. 임베딩

    컴퓨터는 어디까지나 빠르고 효율적인 계산기일 뿐이다. 한마디로 컴퓨터는 인간이 사용하는 자연어를 있는 그대로 이해하는 것이 아니라 숫자를 계산한다는 이야기다. 기계의 자연어 이해와 생성은 연산이나 처리의 영역이다.

    그렇다면 표현력이 무한한 언어를 컴퓨터가 연산할 수 있는 숫자로 바꿀 수 있는 것인가? 만약 그렇다면 말과 글을 숫자로 변환할 때 어떤 정보를 함축시킬 것인가? 그 과정에서 손실이 발생하지는 않을까?

    자연어 처리 분야에서 임베디잉란, 사람이 쓰는 자연어를 기계가 이해할 수 있는 숫자의 나열인 벡터로 바꾼 결과 훅은 그 일련의 과정 전체를 의미한다. 

    임베딩이란 개념은 자연어 처리 분야에서 꽤 오래전부터 사용한 것으로 보이지만, 본격적으로 통용되기 시작한 것은 딥러닝의 대부 요수아 벤지오 연구팀이 [A Neural Probabilistic Language Model] 을 발표하고 나서부터다.

    우리가 상상할 수 있는 가장 간단한 형태의 임베딩은 단어의 빈도를 그대로 벡터로 사용하는 것이다. 아래는 단편 소설들의 단어별 빈도표다. 다음과 같은 빈도표를 단어-문서 행렬(Term-Document Matrix)라고 부른다. 행은 단어, 열은 문서(여기서는 작품)에 대응한다. 

    [단어 문서 행렬](https://www.notion.so/ad8525b33d804207a9a7e71f7e9eabef)

    위의 표를 보면 사랑 손님과 어머니, 삼포가는 길이 사용하는 단어 목록이 상대적으로 많이 겹치고 있는 것을 알 수 있다. 이를 바탕으로 우리는 사랑 손님과 어머니는 삼포 가는 길과 기차라는 소재를 공유한다는 점에서 비슷한 작품일 것이라는 추정을 해볼 수 있다. 또 막걸리와 선술집이라는 단어는 운수 좋은 날에만 등장하는데 이를 바탕으로 우리는 막걸리 - 선술집 간 의미 차이가 막걸리 - 기차 보다 작을 것이라고 추정할 수 있다.

2. 임베딩의 역할

    - 단어/문장 간 관련도 계산

        위의 단어-문서 행렬은 가장 단순한 형태의 임베딩이다. 현업에서는 이보다 복잡한 형태의 임베딩을 사용한다. 대표적인 것이 2013년 구글 연구 팀이 발표한 word2vec 이다. 이름 그대로 단어를 벡터로 바꾸는 방법이다.

        여러 한국어 데이터(네이버 영화 리뷰 말뭉치, 한국어 위키 백과, KorQuAD 등)를 은전한닢으로 형태소 분석을 한 뒤 100차원으로 학습한 word2vec 임베딩을 살펴보자. 의망이라는 단어의 벡터는 다음과 같다.

        $$[-0.00209, -0.03918, 0.02419, ... , 0.01715, -0.04975, 0.09300]$$

        100차원으로 임베딩 했으므로 이 숫자는 모두 100개다. 우리는 위 숫자만 봐서는 그 의미를 알 수 없다. 컴퓨터가 계산하기 좋도록 희망이라는 단어를 벡터로 바꾼 것 뿐이다. 하지만 단어를 벡터로 임베딩하는 순간 단어 벡터들 사이의 유사도(similarity)를 계산하는 일이 가능해진다. 실제로 희망이라는 단어와 유사한 단어를 코사인 유사도를 기준으로 상위 5개를 나열하면 소망, 행복, 희망찬, 꿈, 열망이 나온다.

        임베딩을 수행하면 벡터 공간을 기하학적으로 나타낸 시각화 역시 가능하다. t-SNE 라는 차원 축소(dimension reduction)기법을 사용하면 100차원의 단어 벡터들을 2차원으로 줄여 시각화 할 수 있다. 그러면 관련성이 높은 단어들이 주변에 몰려 있음을 알 수 있다.

    - 의미/문법 정보 함축

        임베딩은 벡터인 만큼 사칙연산이 가능하다. 단어 벡터 간 덧셈/뺄셈을 통해 단어들 사이의 의미적, 문법적 관계를 도출해낼 수 있다. 예를 들면 아들 - 딸 + 소녀 = 소년이 성립하면, 즉, 아들과 딸 사이의 관계와 소년과 소녀 사이의 의미 차이가 임베딩에 함축되어 있으면 성공적인 임베딩이라고 볼 수 있다. 이렇게 단어 임베딩을 평가하는 방법을 단어 유추 평가(word analogy test)라고 부른다.

    - 전이 학습

        임베딩은 다른 딥러닝 모델의 입력값으로 자주 쓰인다. 문서 분류를 위한 딥러닝 모델을 만든다고 해보자. 이럴 때 단어 임베딩은 강력한 힘을 발휘한다. 품질 좋은 임베딩을 쓰면 문서 분류 정확도와 학습 속도가 올라간다. 이렇게 임베딩을 다른 딥러닝 모델의 입력값으로 쓰는 기법을 전이 학습(transfer learning)이라고 한다.

        전이학습은 무언가를 배울 때 0에서 부터 시작하지 않는다. 대규모 말뭉치를 활용해 임베딩을 미리 만들어 놓는다. 임베딩에는 의미적, 문법적 정보 등이 녹아 있다. 이 임베딩을 입력값으로 쓰는 전이 학습 모델은 문서 분류라는 태스크를 빠르게 잘 할 수 있게 된다.

3. 임베딩 기법의 역사와 종류

    - 통계 기반에서 뉴럴 네트워크 기반으로

        초기 임베딩 기법은 대부분 말뭉치의 통계량을 직접적으로 활용하는 경향이 있었다. 대표적인 기법이 잠재 의미 분석(Latent Semantic Analysis)이다. 잠재 의미 분석이란 단어 사용 빈도 등 말뭉치의 통계량 정보가 들어 있는 커다란 행렬에 특이값 분해(singular value decomposition) 등 수학적 기법을 적용해 행렬에 속한 벡트들의 차원을 축소하는 방법을 말한다. 차근차근 알아보자.

        단어-문서 행렬에 잠재 의미 분석을 적용했다고 가정해보자. 그런데 단어-문서 행렬은 보통 행의 개수가 매우 많다. 말뭉치 전체의 어휘 수와 같기 때문이다. (보통 10만-20만 개 내외) 게다가 행렬 대부분의 요소 값은 0이다. 이렇게 대부분의 요소 값이 0인 행렬을 희소 행렬(sparse matrix)라고 한다.

        문제는 이러한 희소 행렬을 다른 모델의 입력값으로 쓰게 되면 계산량도 메모리 소비량도 쓸데없이 커진다는 데 있다. 따라서 원래 행렬의 차원을 축소해 사용한다. 단어를 기준으로 할 수 도 있고, 문서를 기준으로 할 수도 있다.

        이전에는 각각 4개의 문서와 4개의 단어로 표현된 단어-문서 행렬이 있었다면 이를 두 개의 주제만으로 표현하는 것이 가능하다. 단어를 기준으로 삼았다면 단어 수준 임베딩, 문서를 기준으로 삼았다면 문서 임베딩이 된다.

        잠재 의미 분석 수행 대상 행렬은 여러 종류가 될 수 있다. 단어-문서 행렬을 비롯해서 TF-IDF 행렬, 단어-문맥 행렬, 점별 상호 정보량 행렬 등이 바로 그것이다. 잠재 의미 분석 기법은 나중에 다시 다루어 보자.

        최근에는 뉴럴 네트워크 기반의 임베딩 기법들이 주목받고 있다. Neural Probabilistic Language Model 이 발표된 이후부터다. 뉴럴 네트워크 기반 모델들은 이전 단어들이 주어졌을 때 다음 단어가 뭐가 될지 예측하거나 문장 내 일부분을 뚫어 놓고(masking) 해당 단어가 무엇일지 맞추는 과정에서 학습된다.

        뉴럴 네트워크는 그 구조가 유연하고 표현력이 풍부하기 때문에 자연어의 무한한 문맥을 상당 부분 학습할 수 있다. 뉴럴 네트워크 기반의 문장 임베딩 기법 역시 나중에 다루자.

    - 단어 수준에서 문장 수준으로

        2017년 이전의 임베딩 기법들은 대개 단어 수준 모델이었다. NPLM, word2vec, GloVe, FastText, Swivel 등이 여기에 속한다. 단어 임베딩 기법들은 각각의 벡터에 해당 단어의 문맥적 의미를 함축한다. 하지만 단점은 동음이의어를 분간하기 어렵다는 것이다. 

        2018년 초 ELMo가 발표된 이후 문장 수준 임베딩 기법들이 주목을 받았다. BERT나 GPT 등이 여기에 속한다. 문장 수준 임베딩 기법은 개별 단어가 아닌 단어 시퀀스 전체의 문맥적 의미를 함축하기 때문에 단어 임베딩 기법보다 전이 학습 효과가 좋은 것으로 알려져 있다.

    - 룰 → 엔드투엔드 → 프리트레인/파인 튜닝 (pretrain/fine tuning)

        1990년대까지의 자연어 처리 모델 대부분은 사람이 Feature를 직접 뽑았다. Feature란 모델의 입력값을 가리킨다. 한국어 문장을 구문 분석하는 모델을 만든다고 하면 이때 Feature 추출은 언어학적인 지식을 활용한다. 한국어에서의 규칙을 모델에 알려준다.

        2000년대 중반 이후 자연어 처리 분야에서도 딥러닝 모델이 주목받기 시작했다. 딥러닝 모델은 입력과 출력 사이의 관계를 잘 근사하기 때문에 사람이 모델에 규칙을 굳이 직접 알려주지 않아도 된다. 데이터를 통째로 모델에 넣고 입출력 사이의 관계를 사람의 개입 없이 모델 스스로 처음부터 끝까지 이해하도록 유도한다. 이런 기법을 엔드투엔드 모델(end-to-end model) 이라고 부른다. 기계 번역에 널리 쓰였던 Sequence-to-sequnece 모델이 엔드투엔드의 대표 사례다.

        2018년 ELMo 모델이 제안된 이후 자연어 처리 모델은 엔드투엔드 방식에서 벗어나 프리트레인과 파인 튜닝 방식으로 발전하고 있다. 우선 대규모 말뭉치로 임베딩을 만든다 (프리트레인). 이 임베딩에는 말뭉치의 의미적, 문법적 맥락이 포함되어 있다. 이후 임베딩을 입력으로 하는 새로운 딥러닝 모델을 만들고 우리가 풀고 싶은 구체적 문제에 맞는 소규모 데이터에 맞게 임베딩을 포함한 모델 전체를 업데이트한다.(파인 튜닝, 전이 학습). ELMo, GPT, BERT 등이 이 방식에 해당한다. 문장 수준 임베딩을 공부할 때 더 자세히 알아보자.

        우리가 풀고 싶은 자연어 처리의 구체적인 문제들을 다운스트림 태스크(downstream task)라고 한다. 예컨대 품사 판별, 개체명 인식, 의미역 분석 등이 있다. 이에 대비되는 개념이 업스트림 태스크(upstream task)이다. 다운스트림 태스크에 앞서 해결해야 할 과제라는 뜻이다. 문장/단어 임베딩을 프리트레인하는 작업이 바로 업스트림 태스크에 해당한다.

        자연어 처리 분야에서 업스트림과 다운스트림 태스크 사이의 관계는 윗물이 아랫물이 맑을수 있는 것과 같다.

    - 임베딩의 종류와 성능

        우리가 다룰 임베딩 기법은 크게 3가지로 나뉜다. 행렬 분해, 예측, 토픽 기반 방법이 그것이다.

        1. 행렬 분해 기반 방법

            행렬 분해(factorization) 기반 방법은 말뭉치 정보가 들어 있는 원래 행렬을 두 개 이상의 작은 행렬로 쪼개는 방식의 임베딩 기법을 가리킨다. 분해한 이후엔 둘 중하나의 행렬만 쓰거나 둘을 더하거나 이어 붙여 임베딩으로 사용한다. GloVe, Swivel 등이 바로 여기에 속한다.

        2. 예측 기반 방법

            이 방법은 어떤 단어 주변에 특정 단어가 나타날지 예측하거나, 이전 단어들이 주어졌을 때 다음 단어가 무엇일지 예측하거나, 문장 내 일부 단어를 지우고 해당 단어가 무엇일지 맞추는 과정에서 학습하는 방법이다. 뉴럴 네트워크 방법들이 여기에 속한다. word2vec, FastText, BERT, ELMo, GPT 등이 여기에 속한다.

        3. 토픽 기반 방법

            주어진 문서에 잠재된 주제를 추론하는 방식으로 임베딩을 수행하는 기법도 있다. 잠재 디리클레 할당(Latent Dirichlet Allocation)이 대표적인 기법이다. LDA 같은 모델은 학습이 완료되면 각 문서가 어떤 주제 분포를 갖는지 확률 벡터 형태로 반환하기 때문에 임베딩 기법의 일종으로 이해할 수 있다.

        4. 임베딩 성능 평가

            Tenney et al. 은 영어 기반 다운스트림 태스크에 대한 임베딩 종류별 성능을 분석했다. 성능 측정 대상 다운스트램 태스크는 형태소 분석, 문장 성분 분석, 의존 관계 분석, 의미역 분석, 상호 참조 해결 등이다. 파인 튜닝 모델의 구조를 고정한 뒤 각각의 임베딩을 전이 학습 시키는 형태로 정확도를 측정했다. 문장 수준 임베딩 기법인 ELMo, GPT, BERT가 단어 임베딩 기법인 GloVe를 크게 앞서고 있다.