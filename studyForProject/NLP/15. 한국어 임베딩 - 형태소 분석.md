# 한국어 임베딩 - 03 형태소 분석

전처리가 완료된 데이터를 가져왔다. 말뭉치 데이터로는 한국어 위키백과, KorQuAD 학습/데브셋, 네이버 영화 말뭉치 학습/테스트 셋을 사용했다.

품질 좋은 임베딩을 만들기 위해서는 문장이나 단어의 경계를 컴퓨터에 알려줘야 한다. 그렇지 않으면 어휘 집합에 속한 단어 수가 기하급수적으로 늘어나서 연산의 비효율이 발생한다. 특히 한국어는 조사와 어미가 발달한 교착어이기 때문에 이러한 처리를 좀 더 섬세히 해주어야 한다. 예를 들면 '가다'라는 동사는 '가겠다.' '가더라.' 와 같이 다양하게 활용될 수 있다.

위의 '가다'의 활용형을 모두 어휘 집합에 넣는다면 새로운 활용형이 등장할 때 마다 어휘 집합이 계속 늘어난다. 이는 매우 비효율적이다. 이를 해결하기 위해 형태소 분석 기법을 사용해보자.

은전한닢이라는 오픈 소스 형태소 분석기로 가겠다 와 가더라 를 분석한 결과는 '가, 겠, 다', '가, 더라' 이다. 여기에 가겠더라 라는 활용형이 추가되었다고 가정해보자. '가, 겠, 더라'로 분석된다. 따라서 우리는 어휘 집합을 수정하지 않고도 가겠더라 라는 활용형을 처리할 수 있게 된다.

교착어인 한국어는 한정된 종류의 조사와 어미를 자주 이용하기 대문에 각각에 대응하는 명사, 용언 어간만 어휘 집합에 추가하면 취급 단어 개수를 꽤 줄일 수 있다. 형태소 분석기만 잘 활용해도 자연어 처리의 효율성을 높일 수 있다는 이야기다.

1. 지도 학습 기반 형태소 분석

    먼저 지도 학습 기반의 형태소 분석 방법을 설명한다. 여기서 설명하는 형태소 분석기들은 언어학 전문가들이 태깅한 형태소 분석 말뭉치로부터 학습된 지도 학습 기반 모델들이다. 태깅(tagging)이란 아래처럼 모델 입력과 출력 쌍을 만드는 작업을 가리킨다. 이 모델들은 문자열이 주어졌을 때 사람이 알려준 정답 패턴에 최대한 가깝게 토크나이즈한다.

    - 입력 : 아버지가방에들어가신다.
    - 출력: 아버지, 가, 방, 에, 들어가, 신다

    - KoNLPy 사용법

        KoNLPy는 은전한닢, 꼬꼬마, 한나눔, Okt, 코모란 등 5개 오픈소스 형태소 분석기를 파이썬 환경에서 사용할 수 있도록 인터페이스를 통일한 한국어 자연어 패키지다. 각기 다른 언어로 개발된 오픈소스들을 한군데에 묶어 쉽게 사용할 수 있도록 돕는다. 사용법을 분석기별로 순서대로 살펴보자.

        1. 은전한닢

            ```python
            from konlpy.tag import Mecab
            tokenizer = Mecab()
            tokenizer.morphs("아버지가방에들어가신다")
            ```

            결과는 ['아버지', '가', '방', '에', '들어가', '신다'] 이다. 품사 정보까지 확인할 수 있는 코드는 아래와 같다.

            ```python
            tokenizer.pos('아버지가방에들어가신다')
            ```

        2. 나머지

            꼬꼬마, 한나눔, Okt 등 KoNLPy에 속한 다른 형태소 분석기의 사용법은 은전한닢과 동일하다. 다만 처음 tokenizer를 선언할 때만 다르게 해주면 된다.

            (꼬꼬마 : Kkma, 한나눔 : Hannanum, Okt : Okt, 코모란 : Komoran)

            아래는 함수로 어떤 형태소 분석기를 사용하고 싶은지를 인자로 받아 해당 분석기를 반환하는 함수이다. 이후 분석은 위에서 설명한 은전한닢 사용법과 같다.

            함수 아래에는 함수를 활용해 코모란을 사용하는 예시이다.

            ```python
            from konlpy.tag import Okt, Komoran, Mecab, Hannanum, Kkma

            def get_tokenizer(tokenizer_name) :

            	if tokenizer_name == 'komoran' :
            		tokenizer = Komoran()
            	elif tokenizer_name == 'okt' :
            		tokenizer = Okt()
            	elif tokenizer_name == 'mecab' :
            		tokenizer = Mecab()
            	elif tokenizer_name == 'hannanum' :
            		tokenizer = Hannanum()
            	else :
            		tokenizer = Mecab()
            	
            	return tokenizer

            tokenizer = get_tokenizer("komoran")
            tokenizer.morphs("아버지가방에들어가신다")
            tokenizer.pos("아버지가방에들어가신다")
            ```

    - KoNLPy 내 분석기별 성능 차이 분석

        KoNLPy에서는 분석기별 성능 정보 역시 제공하고 있다. 이 정보를 정리한 내용은 다음 표와 같다. 로딩 시간은 분석기가 사용하는 사전 로딩을 포함해 형태소 분석기 클래스를 읽어 들이는 시간을 뜻하고, 실행 시간이란 10만 문자의 문서를 분석하는 데 소요되는 시간을 가리킨다.

        ![table1](./images/table1.png)

        은전 한닢이 다른 분석기 대비 속도가 빠른 것을 확인할 수 있다.

        속도만큼 중요한 지표가 형태소 분석 품질이다. '아버지가방에들어가신다' 문장을 분석한 표는 다음과 같다. 어떤 형태소 분석기를 사용할지는 자신이 가진 데이터로 시험 삼아 형태소 분석을 해보고 속도나 품질을 비교해서 고르는 것이 좋다.

        ![table2](./images/table2.png)

    - Khaiii 사용법

        Khaiii는 카카오가 2018년 말 공개한 오픈소스 한국어 형태소 분석기다. 국립국어원이 구축한 세종 코퍼스를 이용해 CNN 모델을 적용해 학습했다.

        khaiii는 입력 문장을 문자 단위로 읽어 들인 뒤 컨볼루션 필터가 이 문자들을 슬라이등해 가면서 정보를 추출한다. 이 네트워크의 말단 레이어에서는 이렇게 모든 정보들을 종합해 여태소의 경계와 품사 태그를 예측한다. 카카오 측 설명에 따르면 모델을 C++로 구현해 GPU 없이도 형태소 분석이 가능하며 실행 속도 역시 빠르다고 한다.

        [https://lsjsj92.tistory.com/408](https://lsjsj92.tistory.com/408)

        pip install 로는 다운로드가 단되며 위 링크에서 설치 방법을 설명하고 있지만 내 노트북에서는 안된다...

        ```python
        from khaiii import KhaiiiApi

        tokenizer = KhaiiiApi()

        data = tokenizer.analyze("아버지가방에들어가신다")
        tokens = []

        for word in data :
        	tokens.extend([str(m).split("/")[0] for m in word.morphs])
        ```

        결과는 ['아버지', '가', '방에', '들', '어', '가', '시', 'ㄴ다'] 이다. 위 코드에서 맨 마지막 줄에 split("/")을 사용하지 않으면 품사 정보까지 확인할 수 있다.

    - 은전한닢에 사용자 사전 추가하기

        형태소 분석기를 사용하다 보면 가장 신경써야 하는 점이 중요 토큰들을 어떻게 처리해야 할지다. 예컨대 우리가 '가우스전자'라는 기업의 데이터 분석 팀에 속해 있고 가우스전자에 관한 말뭉치를 분석하거나 이로부터 임베딩을 만들어야 한다고 가정해보자. 이 경우 가우스전자 라는 토큰은 섬세하게 처리해야 한다. 은전한닢 분석기로 가상의 상품명 '가우스전자 텔레비전 정말 좋네요'를 분석한 결과는 다음과 같다.

        ```python
        from konlpy.tag import Mecab

        tokenizer = Mecab()
        tokenizer.morphs("가우스전자 텔레비전 정말 좋네요")
        ```

        ['가우스', '전자', '텔레비전', '정말', '좋네요']

        우리의 관심 단어인 가우스전자 가 의도치 않게 두 개의 토큰으로 분석된 것을 확인할 수 있다. 가우스전자 하나로 분석됐을 때보다 데이터 분석이나 임베딩 품질이 떨어질 가능성이 높다. 따라서 우리는 관심 단어들을 사용자 사전에 추가해 가우스전자 같은 단어가 하나의 토큰으로 분석될 수 있도록 강제해야 한다.

        이 절에서 널리 쓰이고 있는 형태소 분석기인 은전한닢을 기준으로 사용자 사전을 추가하는 방법을 살펴보겠다. 추가 방법은 간단하다. preprocess폴더에 mecab-user-dic.csv파일을 하나 만들고 단어를 다음과 같이 추가한다.

        ```
        가우스전자,,,,NNP,*,T,가우스전자,*,*,*,*,*
        서울대입구역,,,,NNP,*,T,서울대입구역,*,*,*,*
        ```

        이후 다음을 파이썬 콘솔에서 실행한다.

        ```
        bash preprocess.sh mecab-user-dic
        ```

2. 비지도 학습 기반 형태소 분석

    비지도 학습 기반의 형태소 분석 방법을 설명한다. 지도 학습 기반은 전문가들이 직접 형태소 경계나 품사 정보를 모델에 가르쳐줘서 학습된 모델들이다. 이와 달리 비지도 학습 기법들은 데이터의 패턴을 모델 스스로 학습하게 함으로써 형태소를 분석하는 방법이다. 데이터에 자주 등장하는 단어들을 형태소로 인식한다.

    - soynlp 형태소 분석기

        위 형태소 분석기는 pip install soynlp로 설치 가능하다. 이는 형태소 분석, 품사 판별 등을 지원하는 파이썬 기반 한국어 자연어 처리 패키지다. 데이터 패턴을 스스로 학습하는 비지도 학습 접근법을 지향하기 때문에 하나의 문장 혹은 문서에서보다는 어느정도 규모가 있으면서 동질적인 문서 집합에서 잘 작동한다.
        
        soynlp 패키지에 포함된 형태소 분석기는 데이터의 통계량을 확인해 만든 단어 점수 표로 작동한다. 단어 점수는 크게 응집 확률과 브랜칭 엔트로피를 활용한다. 조금 더 구체적으로 이야기하면 주어진 문자열이 유기적으로 연결돼 함께 자주 나타나고 (응집 확률이 높을 때), 그 단어 앞뒤로 다양한 조사, 어미 혹은 단어가 등장하는 경우(브랜칭 엔트로피가 높을 때) 해당 문자열을 형태소로 취급한다.
        
        예컨대 말뭉치에서 꿀잼이라는 단어가 연결돼 자주 나타났다면 꿀잼을 형태소라고 본다. (응집 확률이 높음) 한편 꿀잼 앞에 영화, 정말, 너무 등 문자열이, 뒤에 ㅋㅋ, ㅎㅎ, !! 등 패턴이 다양하게 나타났다면 이 역시 꿀잼을 형태소로 취급한다. (브랜칭 엔트로피가 높음)
        
        sonlpy의 형태소 분석기는 우리가 가지고 있는 말뭉치의 통계량을 바탕으로 하기 때문에 별도의 학습 과정을 거쳐야 한다. 말뭉치의 분포가 어떻게 되어 있는지 확인하고 단어별로 응집 확률과 브랜칭 엔트로피 점수표를 만드는 절차가 필요하다는 이야기다. sonlpy의 단어 점수를 학습하는 코드는 다음과 같다. WordExtractor 클래스의 입력 타입은 하나의 요소가 문서인 리스트다. 
        
        아래 코드는 단어 점수를 학습하고 학습한 내용을 model_fname으로 저장하는 코드이다.

        ```python
        from soynlp.word import WordExtractor

        corpus_fname = './processed/processed_ratings.txt'
        model_fname = './processed/soyword_practice.model'

        sentences = [sent.strip() for sent in open(corpus_fname, 'r', encoding='UTF8').readlines()]
        word_extractor = WordExtractor(min_frequency=100,
                                       min_cohesion_forward=0.05,
                                       min_right_branching_entropy=0.0)
        word_extractor.train(sentences)
        word_extractor.save(model_fname)
        ```

        soynlp의 LTokenizer 클래스는 입력 문장의 왼쪽부터 문자 단위로 슬라이딩해 가면서 단어 점수가 높은 문자열을 우선으로 형태소로 취급해 분리한다. 단 띄어쓰기가 되어 있다면 해당 어절을 단어로 인식한다. 한국어는 명사에 조사가 붙거나 용언에 어미가 붙어 활용되는 교착어이기 때문에 왼쪽부터 슬라이딩해 가면서 분석해도 높은 품질을 기대할 수 있다.

        만약 학습을 통해 얻은 단어 점수 표가 ['애비' : 0.5, '종' : 0.4] 라고 한다면 애비는 종이었다 라는 문자열은 ['애비', '는', '종', '이었다'] 로 분석된다.

        위의 코드로 단어 점수 표를 얻었다면 이를 활용해 문장 하나를 형태소 분석해보자.

        ```python
        import math
        from soynlp.word import WordExtractor
        from soynlp.tokenizer import LTokenizer

        model_fname = './processed/soyword_practice.model'

        word_extractor = WordExtractor(min_frequency=100,
                                       min_cohesion_forward=0.05,
                                       min_right_branching_entropy=0.0)
        word_extractor.load(model_fname)

        scores = word_extractor.word_scores()
        scores = {key:(scores[key].cohesion_forward * math.exp(scores[key].right_branching_entropy)) for key in scores.keys()}
        tokenizer = LTokenizer(scores=scores)

        tokens = tokenizer.tokenize("애비는 종이었다")
        ```

        print함수로 tokens를 출력하면 '애비는', '종이었다'가 출력된다.

    - 구글 센텐스피스

        센텐스피스(sentencepiece)는 구글에서 공개한 비지도 학습 기반 형태소 분석 패키지다. 1994년 제안된 바이트 페어 인코딩(BPE)기법 등을 지원하며 pip install sentencepiece를 통해 파이썬 콘솔에서도 사용 가능하다.

        BPE의 기본 원리는 말뭉치에서 가장 많이 등장한 문자열을 병합해 문자열을 압축하는 것이다. 예컨대 우리가 가진 데이터가 다음과 같다고 하자.

        - aaabdaaabac

        이 문자열에서 aa가 가장 많이 나타났다. 이를 Z로 치환하면 원래 문자열을 다음과 같이 압축할 수 있다.

        - ZabdZabac

        위 문자열을 또 한번 압축할 수 있다. ab가 가장 많이 나타났으므로 이를 Y로 치환한다.

        - ZYdZYac

        BPE를 활용해 토크나이즈하는 메커니즘의 핵심은 다음과 같다. 우선 원하는 어휘 집합 크기가 될 때까지 반복적으로 고빈도 문자열들을 병합해 어휘 집합에 추가한다. 이것이 BPE 학습이다.

        학습이 끝난 이후의 예측 과정은 다음과 같다. 문장 내 각 어절 (띄어쓰기로 문장을 나눈 것)에 어휘 집합에 있는 서브워드가 포함되어 있을 경우 해당 서브워드를 어절에서 분리한다. (최장 일치 기준) 이후 어절의 나머지에서 어휘 집합에 있는 서브워드를 다시 찾고 또 분리한다. 어절 끝까지 찾았는데 어휘 집합에 없으면 미등록 단어로 취급한다.

        예컨대 BPE를 학습한 결과 고빈도 서브워드가 학교, 밥, 먹었 등이라고 가정해보자. 그러면 아래 문장은 다음과 같이 분석된다. _로 시작하는 토큰은 해당 토큰이 어절의 시작임을 나타내는 구분자다.

        - 학교에서 밥을 먹었다 → _학교, 에서, _밥, 을, _먹었, 다

        한국어 위키백과 데이터를 가지고 BPE 알고리즘으로 어휘 집합(단어 사전)을 만들어 보자. 아래 코드를 따라 치면 되는데 상당히 오래 걸린다.

        ```python
        import sentencepiece as spm
        train = """--input=./processed/processed_wiki_ko.txt \
                    --model_prefix=sentpiece \
                    --vocab_size=32000 \
                    --model_type=bpe --character_coverage=0.9995"""

        spm.SentencePieceTrainer.Train(train)
        ```

    - 띄어쓰기 교정

        soynlp에서는 띄어쓰기 교정 모듈도 제공한다. 이 모듈은 말뭉치에서 띄어쓰기 패턴을 학습한 뒤 해당 패턴대로 교정을 수행한다. 예컨대 학습 데이터에서 '하자고' 문자열 앞 뒤로 공백이 다수 발견됐다고 예측 단계에서 '하자고'가 등장했을 경우 하자고 앞 뒤를 띄어서 교정하는 방식이다. soynlp의 띄어쓰기 교정 모델 역시 데이터의 통계량을 확인해야 하기 때문에 교정을 수행하기 전 학습이 필요하다.

        전처리된 네이버 영화 리뷰 말뭉치를 활용해 soynlp띄어쓰기 모듈을 학습해보자.

        ```python
        from soyspacing.countbase import CountSpace

        corpus_fname = './processed/processed_ratings.txt'
        model_fname = './processed/space-correct.model'

        model = CountSpace()
        model.train(corpus_fname)
        model.save_model(model_fname, json_format=False)
        ```

        위의 학습된 모델로 띄어쓰기 교정을 수행하는 코드와 결과는 다음과 같다.

        ```python
        from soyspacing.countbase import Countspace

        model_fname = '/processed/space-correct.model'
        model = CountSpace()
        model.load_model(model_fname, json_format=False)
        model.correct('어릴때보고 지금다시봐도 재밌어요')
        ```

        - 어릴 때 보고 지금 다시봐도 재밌어요

        soynlp 형태소 분석이나 BPE 방식의 토크나이즈 기법은 띄어쓰기에 따라 분석결과가 크게 달라진다. 따라서 이들 모델을 학습하기 전 띄어쓰기 교정을 먼저 적용하면 그 분석 품질이 개선될 수 있다.