# 한국어 임베딩 - 09 단어 수준 임베딩 Swivel

Swivel은 구글 연구 팀이 발표한 행렬 분해 기반의 단어 임베딩 기법이다. PMI행렬을 U와 V로 분해하고 학습이 종료되면 U를 단어 임베딩으로 사용할 수 있다. 이밖에 U + V의 전치행렬, U와 V의 전치행렬을 이어 붙여 임베딩으로 사용하는 것도 가능하다.

1. 모델의 기본 구조

    Swivel은 PMI행렬을 분해한다는 점에서 단어-문맥 행렬을 분해하는 GloVe와 다르다. Swivel은 목점함수를 PMI의 단점을 극복할 수 있도록 설계했다는 점이 독특하다. 목점함수는 i라는 타깃 단어와 j라는 문맥 단어가 사용자가 정한 위도우 내에서 단 한 건이라도 동시에 등장한 적이 있는 경우에 적용된다.

    타깃 단어 i에 대응하는 Ui (U행렬에서 i에 대응하는 부분) 벡터와 문맥 단어 j에 해당하는 Vj( V에서 j에 대응하는 부분) 벡터의 내적이 두 단어의 PMI 값과 일치하도록 두 벡터를 조금씩 업데이트한다. 이 때 i와 j의 등장 빈도가 클수록 Ui와 Vj의 벡터 내적 값이 실제 PMI 값과 좀 더 비슷해야 학습 손실이 줄어든다. 다시 말해 단어 i, j가 같이 자주 등장할수록 두 단어에 해당하는 벡터의 내적이 PMI 값과 일치하도록 더욱 강요된다는 이야기다.

    위의 목적 한수는 i와 j가 한 번이라도 동시에 등장한 적이 있을 때 사용했던 목적함수라면 동시에 등장한 적이 한 번도 없는 경우에 적용되는 목적함수도 있다. 두 단어가 한 번도 동시에 등장하지 않으면 PMI는 음의 무한대로 발산하는데 이를 위해 목적함수를 별도로 설정한 것이다.

    단어 i와 j가 고빈도 단어인데 동시 등장 빈도가 0이라면 두 단어는 정말로 같이 등장하지 않는, 의미상 무관계한 단어일 것이라고 가정한다. 예를 들어 '무모'라는 단어와 '운전' 이라는 단어는 단독으로는 자주 등장하지만 두 단어가 연속적으로 쓰이는 경우는 거의 없다. 이럴 땐 두 단어에 해당하는 벡터의 내적 값이 PMI보다 (단 한 번 같이 등장했다고 가정했을 때와 비교해서) 약간 작게 되도록 학습한다. 결과적으로 Ui와 Vj의 내적 값이 작아지게 해야 한다.

    단어 i와 j가 저빈도 단어인데 두 단어의 동시 등장 빈도가 0이라면 두 단어는 의미상 관계가 일부 있을 수 있다고 봤다. 예를 들어 '확률'이라는 단어와 '분포'라는 단어는 아주 흔하지는 않지만 통계학과 관련 있는 데이터에서는 자주 등장하는 편이다. 그런데 우리가 확보한 데이터에서는 이들의 동시 등장 빈도가 0이라고 가정해보자. 이럴 땐 두 단어에 해당하는 벡터의 내적 값이 PMI보다 (단 한 번 같이 등장했다고 가정했을 때와 비교해서) 약간 크게 되도록 학습한다. 결과적으로 Ui와 Vj의 내적 값을 약간 크게 해도 학습 손실이 늘어나지 않는다.

    Swivel은 Glovel와 마찬가지로 U, V 행렬을 랜덤 초기화한 뒤 목적함수를 최소화 하는 방향으로 행렬 값들을 조금씩 업데이트하는 방식으로 학습한다.

2. 튜토리얼

    제시된 파이썬 코드 없음

    [https://github.com/src-d/tensorflow-swivel](https://github.com/src-d/tensorflow-swivel)