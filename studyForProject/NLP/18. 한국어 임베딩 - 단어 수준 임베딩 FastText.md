# 한국어 임베딩 - 06 단어 수준 임베딩 FastText

FastText는 페이스북에서 개발해 공개한 단어 임베딩 기법이다. FastText는 각 단어를 문자 단위 n-gram으로 표현한다. 이 밖의 내용은 Word2Vec과 같다.

1. 모델 기본 구조

    예컨대 '시나브로'라는 단어의 문자 단위 n-gram은 다음과 같다. (n이 3일 때)

    - <시나, 시나브, 나브로, 브로>, <시나브로>

    <와 >는 단어의 경계를 나타내 주기 위해 FastText 모델이 사용하는 특수 기호다. FastText 모델에서는 시나브로라는 단어의 임베딩을 위에서 표현한 문자 단위 n-gram 단어의 벡터 5개의 합으로 나타낸다. (시나브로 = <시나 + 시나브 + 나브로 + 브로> + <시나브로>)

    FastText 모델 역시 네거티브 샘플링 기법을 쓴다. 입력 단어 쌍이 실제 포지티브 샘플이라면 모델은 해당 입력 쌍이 포지티브라고 맞춰야 한다. 여기까지는 Word2Vec과 다르지 않다. FastText는 한발 더 나아가 타깃 단어, 문맥 단어 쌍을 학습할 때 타깃 단어에 속한 문자 단위 n-gram 벡터들을 모두 업데이트한다.

    포지티브 샘플이 주어졌을 때 그에 해당하는 조건부확률을 최대화하려면 분모를 최소화해야 하고 이를 위해서는 n-gram을 표현하는 벡터 z와 문맥 단어 벡터 v간 내적 값을 높여야 한다. 이는 코사인 유사도와 비례한다. 따라서 내적 값의 상향은 타깃 단어에 속하는 문자 단위 n-gram 벡터와 문맥 단어의 포지티브 샘플에 해당하는 단어 벡터 간 유사도를 높여야 한다고 이해할 수 있다.

    또한 입력 값이 네거티브 샘플이 주어졌을 때 입력 값이 네거티브 샘플인 것을 맞추어야 하므로 이를 표현하는 조건부확률을 높여야 한다. 그러기 위해서는 z와 v 간 내적 값을 낮추어야 하고, 이는 코사인 유사도를 떨어뜨려야 한다고 이해할 수 있다. 결과적으로 벡터 공간에서 멀어지게 된다.

2. 튜토리얼

    책에는 임베딩을 직접 해보는 코드가 도커를 이용하는데 우리는 지금 할 수가 없으므로 검색을 해 본 결과 from gensim.models import FastText 으로 임포트를 한 후 Word2Vec과 거의 동일하게 사용하면 되는 것을 알 수 있었다. Word2Vec() 대신 FastText()를 사용한다.

    FastText 모델의 강점은 조사나 어미가 발달한 한국어에 좋은 성능을 낼 수 있다는 점이다. 실제로 FastText 모델로 학습하면 용언의 활용이나 그와 관계된 어미들이 벡터 공간상 가깝게 임베딩된다.

    FastText는 또한 오타나 미등록 단어에도 강건하다. 임베딩을 문자 단위의 n-gram 벡터의 합으로 표현하기 때문이다. 만약 미등록 단어 서울특별시 가 있다고 해도 FastText 임베딩을 추정할 수 있다. '서울' 같은 문자 단위 n-gram이 다른 n-gram이나 단어들에 있을 확률이 높기 때문이다. 나머지 '울특', '특별', '별시' 등이 모두 미등록 단어라 할지라도 서울특별시에 대한 임베딩을 추정할 수 있는 것이다. 다른 단어 임베딩 기법이 미등록 단어 벡터를 아예 추출할 수 없다는 사실을 감안하면 FastText는 경쟁력이 있다.

3. 한글 자소와 FastText

    FastText는 문자 단위 n-gram을 쓰기 때문에 한글과 궁합이 잘 맞는 편이다 .한글은 자소 단위로 분해할 수 있고, 이 자소 각각을 하나의 문자로 보고 FastText를 시행할 수 있기 때문이다.

    문자들을 자소단위로 먼저 분해한 말뭉치를 사용한다는 점을 빼면 FastText와 코드가 같다. 이 경우 FastText모델은 각 자소를 하나의 문자로 보고 문자 단위 n-gram 임베딩을 한다. 또 마지막으로 이 단어들의 자소를 합쳐 다시 원래 단어로 복원시킨다.

    이러한 방법이 있다는 것을 알아두는 선에서 이해하고 나중에 직접 구현해볼 때 더 자세히 찾아보자.