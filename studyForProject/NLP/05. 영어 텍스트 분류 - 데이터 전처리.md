# 영어 텍스트 분류 - 01 데이터 전처리



텍스트 분류에는 여러 활용 사례가 있지만, 그 중에서 이번 장에서는 감정 분류 문제를 다루어 보자. 주어진 글을 분석한 후 감정을 긍정 혹은 부정으로 예측하는 모델을 만들 예정이다. 여기서는 긍정 혹은 부정으로만 구분해서 분류하지만, 경우에 따라 중립의 감정이 있을 수 있고, 긍정, 부정 역시 세분화되어 나누어질 수 있다. 영어가 데이터와 공부할 자료가 많고, 조금 더 쉬우므로 영어를 먼저 공부하고, 그 다음 한국어로 해보자.

데이터 : https://www.kaggle.com/c/word2vec-nlp-tutorial/data

- 목표
    1. 데이터를 불러오는 것과 정제되지 않은 데이터를 활용하기 쉽게 전처리 하는 과정
    2. 데이터를 분석하는 과정
    3. 실제로 문제를 해결하기 위해 알고리즘을 모델링하는 과정

- 데이터의 처리 과정
    1. 데이터를 불러오기
    2. EDA(탐색적 자료 분석)
    3. 데이터 정제 (지금 뭔지 몰라도 된다.)
        - HTML 및 문장 부호 제거
        - 불용어 제거
        - 단어 최대 길이 설정
        - 단어 패딩
        - 벡터 표상화
    4. 모델링

- 데이터 불러오기와 EDA

    ```python
    import os
    import numpy as np
    import pandas as pd
    import matplotlib.pyplot as plt
    import seaborn as sns
    from wordcloud import WordCloud
    %matplotlib inline
    ```

    필요한 라이브러리를 미리 불러오자. wordcloud 실행이 안되면 pip install wordcloud 를 쳐서 깔아놓자.

    ```python
    train_data = pd.read_csv('labeledTrainData.tsv', header=0, delimiter="\t", quoting=3)
    train_data.head()
    ```

    훈련 데이터를 불러오고 확인한다. 훈련데이터와 작성되는 코드파일이 한 폴더안에 있어야 한다.

    ```python
    print("전체 학습 데이터의 개수 : {}".format(len(train_data)))
    ```

    전체 학습 데이터 개수를 확인한다. 25000개임을 알 수 있다.

    ```python
    train_length = train_data['review'].apply(len)
    train_length.head()
    ```

    리뷰의 길이를 알아보자. 리뷰의 길이를 새로운 변수로 따로 저장해두었다. apply라는 메소드에 있는 len이 앞에있는 train_data['review']에 적용되어 train_length변수에 저장된다. 즉, 하나하나의 리뷰들마다 len이 적용된 채로 train_length에 저장되는 것이다.

    ```python
    # 이 변수를 사용해 히스토그램
    plt.figure(figsize=(12, 5))
    plt.hist(train_length, bins=200, alpha=0.5, color='r', label='word')
    plt.yscale('log', nonposy='clip')
    plt.title('Log-Histogram of length of review')
    plt.xlabel('Length of review')
    plt.ylabel('Number of review')
    plt.show()
    ```

    히스토그램을 통해 리뷰들의 길이들이 어떻게 분포해있는지 확인해볼 수 있다.

    ```python
    print('리뷰 길이 최대 값: {}'.format(np.max(train_length)))
    print('리뷰 길이 최소 값: {}'.format(np.min(train_length)))
    print('리뷰 길이 평균 값: {:.2f}'.format(np.mean(train_length)))
    print('리뷰 길이 표준편차: {:.2f}'.format(np.std(train_length)))
    print('리뷰 길이 중간 값: {}'.format(np.median(train_length)))
    # 사분위의 대한 경우는 0~100 스케일로 되어있음
    print('리뷰 길이 제 1 사분위: {}'.format(np.percentile(train_length, 25)))
    print('리뷰 길이 제 3 사분위: {}'.format(np.percentile(train_length, 75)))
    ```

    리뷰들의 길이에 대해 수치로 확인한다.

    ```python
    # 박스 플롯 생성
    plt.figure(figsize=(12, 5))
    plt.boxplot(train_length, labels=['counts'], showmeans=True) #showmeans -> 평균값을 표시함
    plt.show()
    ```

    박스플롯을 생성해 확인해본다. 이상치가 많다는 것을 알 수 있다.

    ```python
    cloud = WordCloud(width=800, height=600).generate(" ".join(train_data['review']))
    plt.figure(figsize=(12, 5))
    plt.imshow(cloud)
    plt.axis('off')
    ```

    wordcloud를 통해 어떤 단어들이 많은지 알 수 있다. br이라는 단어가 많이 등장하는데, br은 웹에서 엔터키를 하는 역할의 태그 이름이다. 일반적으로 우리가 사용하는 말에 br을 사용하는 경우는 없으니 나중에 제거시켜줘야 함을 알 수 있다.

    ```python
    fig, axe = plt.subplots(ncols=1)
    fig.set_size_inches(6, 3)
    sns.countplot(train_data['sentiment'])

    print("긍정 리뷰 개수: {}".format(train_data['sentiment'].value_counts()[1]))
    print("부정 리뷰 개수: {}".format(train_data['sentiment'].value_counts()[0]))
    ```

    리뷰의 긍정 부정의 분포를 알아본다. 정확히 50대 50 비율로 존재함을 알 수 있다. 만약 긍정 부정 분포가 균일하지 않고 8대 2, 9대 1 처럼 비율이 깨지면 학습의 효율성이 떨어진다.

    ```python
    train_word_counts = train_data['review'].apply(lambda x : len(x.split(' ')))

    plt.figure(figsize=(15, 10))
    plt.hist(train_word_counts, bins=50, facecolor='r', label='train')
    plt.title('Log-Histogram of word count in review', fontsize=15)
    plt.yscale('log', nonposy='clip')
    plt.xlabel('Number of words', fontsize=15)
    plt.ylabel('Number of reviews', fontsize=15)
    plt.legend()
    plt.show()
    ```

    각 리뷰들의 길이를 살펴 보았으니, 리뷰를 이루고 있는 단어들의 개수에 대해 살펴보자. 첫 줄이 리뷰들을 단어 단위로 쪼개서 새 변수에 담아주는 코드이다. 그래프의 x축이 '리뷰에 들어있는 단어의 개수'이고, y축이 '리뷰들의 개수'이다.

    ```python
    print('리뷰 단어 개수 최대 값: {}'.format(np.max(train_word_counts)))
    print('리뷰 단어 개수 최소 값: {}'.format(np.min(train_word_counts)))
    print('리뷰 단어 개수 평균 값: {:.2f}'.format(np.mean(train_word_counts)))
    print('리뷰 단어 개수 표준편차: {:.2f}'.format(np.std(train_word_counts)))
    print('리뷰 단어 개수 중간 값: {}'.format(np.median(train_word_counts)))
    # 사분위의 대한 경우는 0~100 스케일로 되어있음
    print('리뷰 단어 개수 제 1 사분위: {}'.format(np.percentile(train_word_counts, 25)))
    print('리뷰 단어 개수 제 3 사분위: {}'.format(np.percentile(train_word_counts, 75)))
    ```

    리뷰 단어들에 대한 수치를 확인한다.

    ```python
    qmarks = np.mean(train_data['review'].apply(lambda x: '?' in x)) 
    # 물음표가 구두점으로 쓰임
    fullstop = np.mean(train_data['review'].apply(lambda x: '.' in x)) 
    # 마침표가 구두점으로 쓰임
    capital_first = np.mean(train_data['review'].apply(lambda x: x[0].isupper())) 
    # 첫번째 문자가 대문자
    capitals = np.mean(train_data['review'].apply(lambda x: max([y.isupper() for y in x]))) # 대문자가 몇개
    numbers = np.mean(train_data['review'].apply(lambda x: max([y.isdigit() for y in x]))) # 숫자가 몇개
                      
    print('물음표가있는 질문: {:.2f}%'.format(qmarks * 100))
    print('마침표가 있는 질문: {:.2f}%'.format(fullstop * 100))
    print('첫 글자가 대문자 인 질문: {:.2f}%'.format(capital_first * 100))
    print('대문자가있는 질문: {:.2f}%'.format(capitals * 100))
    print('숫자가있는 질문: {:.2f}%'.format(numbers * 100))
    ```

    마지막으로 각 리뷰에 대해 구두점과 대소문자 비율을 확인한다. 컴퓨터는 대문자와 소문자를 다른 문자로 인식하기에 알아둘 필요성이 있다. 결과를 보면 대부분 마침표를 포함하고 있고, 대문자도 대부분 사용하고 있다. 따라서 전처리 과정에서 대문자의 경우 모두 소문자로 바꾸고 특수 문자의 경우 제거한다. 이 과정은 학습에 방해가 되는 요소들을 제거하기 위함이다.

- 데이터 전처리

    이제부터 데이터들을 학습시키기 좋게 데이터 전처리 과정을 진행할 것이다. 그런데, 데이터 전처리는 학습데이터에 맞추어 해주고 테스트 데이터에는 맞추어주지 않는다. 그 이유는 인공지능은 실전에서 어떤 데이터가 들어오는 지 모르기 때문에 학습데이터와 똑같은 전처리 과정을 거친 다음 인공지능에 입력시키고, 결과를 돌려받게 된다. 테스트 데이터는 실전에서의 정확성이 얼마나 되는지 측정하기 위한 데이터이므로 테스트 데이터만을 위해 따로 전처리를 해준다면 테스트를 하는 의미가 없게 된다. 그래서 테스트 데이터에는 테스트 데이터의 특성을 살린 전처리가 아닌 학습 데이터와 동일한 전처리 과정을 실행시켜준다. 아무튼 그런 이유로 우선 먼저 학습 데이터만 전처리해보자.

    ```python
    import re
    import numpy as np
    import pandas as pd
    import json
    from bs4 import BeautifulSoup
    from nltk.corpus import stopwords
    from tensorflow.python.keras.preprocessing.sequence import pad_sequences
    from tensorflow.python.keras.preprocessing.text import Tokenizer
    ```

    필요한 라이브러리를 불러오자. bs4가 안불러와지면 pip install beatifulsoup4 로 설치하고, nltk가 안불러와지만 pip install nltk로 설치하자.

    ```python
    train_data = pd.read_csv('labeledTrainData.tsv', header=0, delimiter='\t', quoting=3)
    print(train_data['review'][0])
    ```

    학습 데이터를 불러오고 확인해보자. 리뷰의 길이가 꽤 긴 것을 알 수 있다. 그런데 리뷰를 보면 <br /> 이 계속 등장하는 것을 알 수 있는데, HTML5에서 줄바꿈 (엔터 키)의 역할을 하는 태그이다. 이전에 본 wordcloud에서 br이 많이 나온 것이 이 것 때문이다.

    ```python
    review = train_data['review'][0]
    #리뷰 데이터를 하나만 가져온다.
    review_text = BeautifulSoup(review, "html5lib").get_text()
    #<br />를 없애는 역할을 하는데, 정확한 설명이 궁금하면
    #다음에 만나서 알려줄게
    review_text = re.sub("[^a-zA-z]", " ", review_text)
    #정규표현식이라는 것을 써서 영어 소문자, 대문자가 아닌 것들은 모두
    #공백으로 바꾸는 코드이다.
    print(review_text)
    #잘 처리되었는지 출력해본다.
    ```

    우선 리뷰 하나에 대해서 HTML5태그와 특수문자 등은 없애주었다. 그 다음은 Stopwords제거이다. Stopwords는 불용어라고도 하는데, 우리가 쓰는 말에서 필요한 단어이지만 큰 의미가 없는 단어들을 말한다. 경우에 따라 지정해줄 수도 안해줄 수도 있고, 사용자가 직접 불용어 리스트를 작성해 기초 파이썬 코드로 제거해줄 수도 있다. 여기서는 불용어를 제거한다고 해서 감정 판단에 큰 영향을 주지 않는다고 가정하고 nltk라이브러리에 미리 있는 불용어 사전을 이용하자.

    ```python
    nltk.download('stopwords')

    stop_words = set(stopwords.words('english')) 
    # 영어 불용어들의 set을 만든다.

    review_text = review_text.lower()
    words = review_text.split()
    # 여기서는 단어를 공백을 기준으로 나누었다.
    # words에는 공백을 기준으로 잘린 단어들이 차례로 리스트형태로 저장된다.
    # 소문자 변환 후 단어마다 나눠서 단어 리스트로 만든다.
    words = [w for w in words if not w in stop_words] 
    # 불용어 제거한 리스트를 만든다
    print(words)
    ```

    두번째 줄의 set()은 새로운 자료형인데 수학시간에 배운 집합과 같은 특성을 가지고 있다. 중복된 값이 들어갈 수 없고, 순서가 없다. 또 모든 단어는 소문자로 만들어준다. 컴퓨터는 소문자와 대문자를 완전히 다른 문자로 인식하기 때문이다. words변수에 첫번째 리뷰의 단어들이 리스트 형태로 불용어가 제거된 단어들이 저장되었다.

    ```python
    clean_review = ' '.join(words)
    print(clean_review)
    ```

    clean_review에 words 리스트에 있는 단어들을 공백을 기준으로 문자열로 이어 붙인 것이다. 위 두 코드의 결과물을 비교해보면 어떤 일이 일어났는지 짐작 가능할 것이다.

    그런데 이 과정은 리뷰 하나에만 이루어졌는데, 이를 25000개의 모든 데이터에 일일히 할 수는 없는 노릇일 것이다. 따라서 위 과정을 함수로 저장해주고, 반복문을 이용해 25000개의 학습데이터를 빠르게 전처리해주자.

    ```python
    def preprocessing( review, remove_stopwords = False ): 
        # 불용어 제거는 옵션으로 선택 가능하다.
        
        # 1. HTML 태그 제거
        review_text = BeautifulSoup(review, "html5lib").get_text()	

        # 2. 영어가 아닌 특수문자들을 공백(" ")으로 바꾸기
        review_text = re.sub("[^a-zA-Z]", " ", review_text)

        # 3. 대문자들을 소문자로 바꾸고 공백단위로 텍스트들 나눠서 리스트로 만든다.
        words = review_text.lower().split()

        if remove_stopwords: 
            # 4. 불용어들을 제거
        
            #영어에 관련된 불용어 불러오기
            stops = set(stopwords.words("english"))
            # 불용어가 아닌 단어들로 이루어진 새로운 리스트 생성
            words = [w for w in words if not w in stops]
            # 5. 단어 리스트를 공백을 넣어서 하나의 글로 합친다.	
            clean_review = ' '.join(words)

        else: # 불용어 제거하지 않을 때
            clean_review = ' '.join(words)

        return clean_review
    ```

    위 과정은 앞 과정이 거의 반복되있는 것을 알 수 있다. 입력값으로 review와 remove_stopwords가 들어오는데, remove_stopwords=False라고 되어 있는 것은 이 입력값이 필수는 아니고, 옵션으로 선택이 가능하다는 것이다. 또 결과로 전처리된 리뷰가 나오는 것을 알 수 있다. 아무튼 함수를 정의했으니 데이터들을 모두 반복문을 이용해 입력하고 저장해주자.

    ```python
    clean_train_reviews = []
    # 반복문을 돌면서 결과가 나올 때마다 결과를 저장할 변수를 만들고,
    # 빈 리스트를 저장한다.
    for review in train_data['review']:
        clean_train_reviews.append(preprocessing(review, remove_stopwords = True))
    # 반복문을 돌면서 train_data['review'] (전처리가 되지 않은 데이터)들을
    # 입력시켜준다. 이 결과는 clean_train_reviews.append()를 통해
    # 계속 clean_train_reviews에 차곡차곡 저장된다.
    # append()의 입력값으로 위에서 정의한 함수가 들어갔기 때문에
    # 전처리된 데이터들이 빈 리스틀에 저장될 수 있는 것

    # 전처리한 데이터 출력
    clean_train_reviews[0]
    ```

    ```python
    clean_train_df = pd.DataFrame({'review': clean_train_reviews, 'sentiment': train_data['sentiment']})
    # 전처리된 리뷰들을 데이터 프레임 형태로 저장
    ```

    이제 다시 남은 전처리 과정을 진행해보자. 우리가 데이터를 모델에 입력시킬 때 입력값이 텍스트가 아닌 각 단어의 '인덱스'가 되어야 하고, 데이터들이 모두 동일한 길이어야 하는 경우가 있기에 이 과정을 진행한다. 여기서는 텐서플로우의 전처리 모듈을 사용한다.

    ```python
    tokenizer = Tokenizer()
    # 토크나이저 모듈을 불러온다.
    tokenizer.fit_on_texts(clean_train_reviews)
    # 토크나이저에 우리가 전처리한 데이터를 입력한다.
    text_sequences = tokenizer.texts_to_sequences(clean_train_reviews)
    # 단어들을 숫자로 바꾼 뒤 text_sequences에 저장한다.
    print(text_sequences)
    # 확인
    ```

    이렇게 해서 출력된 숫자들이 리뷰를 이루고 있는 각 단어들의 인덱스를 뜻한다. 이 인덱스들이 각각 어떤 단어를 가르키는지 알기 위해, 어떤 단어를 의미하는지 확인하기 위해 단어 사전이 필요하다. 단어 사전을 확인해보자.

    ```python
    word_vocab = tokenizer.word_index
    word_vocab["<PAD>"] = 0
    print("전체 단어 개수: ", len(word_vocab))
    print(word_vocab)
    ```

    word_vocab에 단어 사전을 저장했다. 두번째 줄은 패딩 정보값이 정의되어 있지 않아서 지정해둔 값이라고 하는데, 정확한 의미는 자 모르겠다. 전체 단어 개수는 약 74000개인 것으로 확인된다. 단어 사전뿐 아니라 전체 단어 개수도 이후 모델에서 사용되기 때문에 저장해 둔다. 데이터에 대한 정보인 단어 사전과 전체 단어 개수는 새롭게 딕셔너리 값을 지정해서 저장해두자.

    ```python
    data_configs = {}

    data_configs['vocab'] = word_vocab
    data_configs['vocab_size'] = len(word_vocab)
    ```

    이제 마지막 전처리 과정만 남았다. 현재 각 데이터는 서로 길이가 다르다. 이 길이를 하나로 통일해야 이후 모델에 바로 적용할 수 있기 때문에 특정 길이를 최대 길이로 정하고 더 긴 데이터의 경우 뒷부분을 자르고, 짧은 데이터의 경우에는 0 값으로 패딩하는 작업을 진행한다.

    ```python
    MAX_SEQUENCE_LENGTH = 174 
    # 최대 길이를 174로 정함
    # 이 값은 코딩하는 사람이 앞에서 했던 EDA과정에서 얻은
    # 데이터에 대한 값들을 토대로 정한다.
    # 여기서는 단어 개수의 통계를 계산했을 때 중간값으로 나온 174를 사용했다.
    # 보통 편균이 아닌 중간값을 사용하는 경우가 많은데, 이상치에 의해
    # 평균은 급격히 올라갈 수 있기 때문이다.
    train_inputs = pad_sequences(text_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')
    # 직접 패딩을 해주는 코드이다. 첫번째 인자로 패딩할 데이터(문자를 숫자로 바꾼 데이터),
    # 두번째 인자로 패딩해주는 길이, 세번째로는 패딩의 방식을 설정했다. 정확한 것은 다음에...
    print('Shape of train data: ', train_inputs.shape)
    # 이렇게 전처리된 학습 데이터의 모양을 확인한다. 데이터는 25000개, 길이는 174이므로
    # (25000, 174)의 모양을 하고 있다.
    ```

    이제 마지막으로 학습 시 라벨, 즉 정답을 나타내는 값을 넘파이 배열로 저장한다. 넘파이 배열로 저장하는 이유는 이후 전처리한 데이터를 저장할 때 넘파이 형태로 저장하기 때문이다.

    ```python
    train_labels = np.array(train_data['sentiment'])
    print('Shape of label tensor:', train_labels.shape)
    ```

    넘파이 형태로 저장했고, 모양이 (25000,)인 것을 알 수 있다. 길이가 25000인 벡터이다. 

    나머지 코드는 우리가 작성하고 처리해준 데이터를 저장하는 코드이다.

    ```python
    TRAIN_INPUT_DATA = 'train_input.npy'
    TRAIN_LABEL_DATA = 'train_label.npy'
    TRAIN_CLEAN_DATA = 'train_clean.csv'
    DATA_CONFIGS = 'data_configs.json'
    DATA_IN_PATH = './'

    import os
    # 저장하는 디렉토리가 존재하지 않으면 생성
    if not os.path.exists(DATA_IN_PATH):
        os.makedirs(DATA_IN_PATH)

    # 전처리 된 데이터를 넘파이 형태로 저장
    np.save(open(DATA_IN_PATH + TRAIN_INPUT_DATA, 'wb'), train_inputs)
    np.save(open(DATA_IN_PATH + TRAIN_LABEL_DATA, 'wb'), train_labels)

    # 정제된 텍스트를 csv 형태로 저장
    clean_train_df.to_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA, index = False)

    # 데이터 사전을 json 형태로 저장
    json.dump(data_configs, open(DATA_IN_PATH + DATA_CONFIGS, 'w'), ensure_ascii=False)

    test_data = pd.read_csv(DATA_IN_PATH + "testData.tsv", header=0, delimiter="\t", quoting=3)

    clean_test_reviews = []
    for review in test_data['review']:
        clean_test_reviews.append(preprocessing(review, remove_stopwords = True))

    clean_test_df = pd.DataFrame({'review': clean_test_reviews, 'id': test_data['id']})
    test_id = np.array(test_data['id'])

    text_sequences = tokenizer.texts_to_sequences(clean_test_reviews)
    test_inputs = pad_sequences(text_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')

    TEST_INPUT_DATA = 'test_input.npy'
    TEST_CLEAN_DATA = 'test_clean.csv'
    TEST_ID_DATA = 'test_id.npy'

    np.save(open(DATA_IN_PATH + TEST_INPUT_DATA, 'wb'), test_inputs)
    np.save(open(DATA_IN_PATH + TEST_ID_DATA, 'wb'), test_id)
    clean_test_df.to_csv(DATA_IN_PATH + TEST_CLEAN_DATA, index = False)
    ```