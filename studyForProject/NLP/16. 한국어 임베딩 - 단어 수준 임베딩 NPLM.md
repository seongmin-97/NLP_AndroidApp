# 한국어 임베딩 - 04 단어 수준 임베딩 NPLM

NPLM(Neural Probabilistic Language Model)을 살핀다. NPLM은 자연어 처리 분야에 임베딩 개념을 널리 퍼뜨리는 데 일조한 선구자적 모델이며 임베딩 역사에서 차지하는 역할이 작지 않다.

1. 모델의 기본 구조

    NPLM은 딥러닝의 대부 요슈아 벤지오 연구팀이 제안한 기법이다. (2003, Bengio et al.) NPLM은 앞서 '단어가 어떤 순서로 쓰였는가' 에서 설명한 통계 기반의 전통적인 언어 모델의 한계를 극복하는 과정에서 탄생했다. Bengio et al. (2003) 은 기존 언어 모델의 단점을 다음과 같이 정리했다.

    - 학습 데이터에 존재하지 않는 n-gram이 포함된 문장이 나타날 확률 값을 0으로 부여한다. 물론 백오프나 스무딩으로 이런 문제를 보완할 수 있지만 완전하지는 않다.
    - 문장의 장기 의존성을 포착해내기 어렵다. 다시 말해 n-gram 모델의 n을 5 이상으로 설정할 수 없는데, n이 커질 수록 그 등장 확률이 0인 단어 시퀀스가 기하급수적으로 늘어난다.
    - 단어/문장 간 유사도를 계산할 수 없다.

    NPLM은 이러한 기존 언어 모델의 한계를 일부 극복한 언어 모델이라는 점에서 의의가 있다. 그뿐만 아니라 NPLM 자체가 단어 임베딩 역할을 수행할 수 있다. 

2. NPLM의 학습

    NPLM은 단어 시퀀스가 주어졌을 때 다음 단어가 무엇인지 맞추는 과정에서 학습된다. NPLM은 직전까지 등장한 n-1개의 단어들로 다음 단어를 맞추는 이전에 했던 n-gram 언어 모델이다.

    발, 없는, 말 이라는 세 개의 단어가 주어졌다고 가정하자. 그 다음에 올 단어는 무엇일까? 수많은 단어들이 있겠지만, 우리가 가진 말뭉치에는 '천리' 와 '간다' 라는 단어가 연이어 자주 등장했다고 해보자.

    NPLM은 조건부확률 P(천리|발, 없는, 말이) , P(간다|없는, 말이, 천리) 의 확률을 높이려고 한다. 각각 발, 없는, 말이라는 단어가 왔을 때 천리가 등장할 확률, 없는, 말이, 천리가 등장했을 때 간다가 등장할 확률을 뜻한다.

    NPLM 구조 말단의 출력은 |V|차원의 스코어 벡터 y(wt)에 소프트맥스 함수 (다중 분류 모델에서 자주 사용한다.) 를 적용한 |V| 차원의 확률 벡터이다. 이는 곧 위에서 이야기한 형태의 조건부확률과 같다.

    즉, 단어 시퀀스가 입력되면 그 다음에 올 단어들에 대한 확률값들로 이루어진 벡터가 결과로 출력된다. (어떤 단어가 나올 확률은 X이다.)

    NPLM은 확률 벡터에서 가장 높은 값의 인덱스에 해당하는 단어가 실제 정답 단어와 일치하도록 학습한다.

    이번에는 입력을 살펴보자. 문장 내 t번째 단어 wt에 대응하는 단어 벡터 xt를 만드는 과정이다. |V| x m 크기를 같는 커다란 행렬 C에서 wt에 해당하는 벡터를 참조하는 형태다. |V|는 어휘 집합 크기, m은 xt의 차원 수이다. C 행렬의 원소 값은 초기에 랜덤하게 설정한다.

    여기서 참조는 어떤 의미일까. 어휘 집합에 속한 단어가 5개 뿐이고 wt가 이 가운데 네 번째라고 하면 C(wt)는 행렬 C와 wt에 해당하는 원핫벡터를 내적하는 것과 같다. 원핫벡터란 한 요소만 1이고 나머지는 0인 벡터를 가리킨다. 이는 C라는 행렬에서 wt에 해당하는 행만 참조하는 것과 동일하다. 결과적으로는 C 행렬에서 wt가 가리키는 단어, 예를 들어 5개의 단어 중 4번째 단어라고 하면 C 행렬에서 4번째 행만 참조하는 것이다.

    입력에 없는, 말이, 천리 이렇게 세 개의 단어가 주어졌을 때 간다 라는 단어를 예측해야 한다고 가정하자. 우선 세 개 각각의 단어의 인덱스 값을 확인한다. 그리고 C 행렬에서 세 단어에 해당하는 벡터를 참조한 뒤 이 세 개의 벡터를 묶어주면 NPLM의 입력 벡터 x 가 된다.

    이렇게 입력된 값들은 tanh 함수를 통해 스코어 벡터 y(wt)를 계산하고, 여기에 소프트맥스 함수를 적용한 뒤 정답 단어인 '간다'의 인덱스와 비교해 역전파 하는 방식으로 학습이 이루어진다. NPLM 학습이 종료되면 우리는 행렬 C를 각 단어에 해당하는 m차원 임베딩으로 사용한다.

    NPLM은 이후에 설명하는 단어 임베딩 기법들과 비교하면 학습해야 하는 파라미터 종류가 많고 그 크기 또한 큰 편이다. 이후 제안된 기법은 추정 대상 파라미터를 줄이고 그 품질은 높이는 쪽으로 발전했다.

3. NPLM과 의미 정보

    NPLM은 단어의 의미를 어떻게 임베딩에 녹여낼 수 있는 걸까. Bengio et al. (2003)이 예로 든 다음 문장들을 보자.

    ```
    The cat is walking in the bedroom
    A dog was running in a room
    The cat is running in a room
    A dog is walking in a bedroom
    The dog was walking in the room
    ```

    우선 n-gram 의 n을 4라고 하자. 그러면 NLPM 은 직전 3개 단어를 가지고 그 다음 단어 하나를 맞추는 과정에서 학습된다. 네 번째 단어가 walking 인 문장은 첫 줄, 네 번째 줄, 다섯 번째 줄이다. 따라서 NPLM은 'The cat is', 'A dog is', 'The dog was"를 입력 받으면 walking이 출력되도록 학습한다.

    따라서 The, A, cat, dog, is, was 등은 walking 이라는 단어와 모종의 관계가 형성된다. 바꿔 말하면 위 단어들에 해당하는 C 행렬의 행 백터들은 walking을 맞추는 과정에서 발생한 손실을 최소화하는 그래디언트(gradient)를 받아 동일하게 업데이트된다. 결과적으로는 The, A, cat, dog, is was 벡터가 벡터 공간에서 같은 방향으로 조금 움직인다는 이야기다.

    마찬가지로 네 번째 단어가 running 인 문장을 뽑아 보면 두 번째와 세 번째 문장이다. 따라서 이 두 문장을 이루는 앞부분의 3 단어는 running이라는 단어와 관계를 지니게 되고, 결과적으로 이 단어들은 벡터 공간에서 같은 방향으로 업데이트 된다.

    이런 식으로 문장 내 모든 단어들을 한 단어씩 훑으면서 말뭉치 전체를 학습하게 된다면 NPLM 모델의 C 행렬에 각 단어의 문맥 정보를 내재할 수 있게 된다.

    NPLM은 그 자체로 언어 모델 역할을 수행할 수 있다. 예컨대 학습 데이터에 없는 The mouse is running in a room 이라는 문장의 등장 확률을 예측해야 한다고 할 때 기존의 통계 기반 n-gram은 학습 데이터에 한 번도 등장하지 않은 패턴에 대해서는 등장 확률을 0으로 부여하는 문제점이 있었지만 NPLM은 위 문장이 말뭉치에 없더라도 문맥이 비슷한 다른 문장을 참고해 확률을 부여한다. 결과적으로 NPLM은 위 문장의 등장 확률을 다른 문장들과 비슷하게 추론하게 되는 것이다.