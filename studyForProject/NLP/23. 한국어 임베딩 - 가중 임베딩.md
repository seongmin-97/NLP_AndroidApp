# 한국어 임베딩 - 11 가중 임베딩

이번에는 단어 임베딩을 문장 수눙 임베딩으로 확장하는 방법을 알아보자. 간단하지만 성능 향상 효과가 좋아 한번 써 봄직하다. 기국 프린스턴대학교 연구 팀이 ICLR에 발표한 방법론이다. (2016년)

1. 모델 개요

    연구팀은 문서 내 단어의 등장은 저자가 생각한 주제에 의존한다고 가정했다. 다시 말해 주제에 따라 단어의 사용 양상이 달라진다는 것이다. 이를 위해 연구 팀은 주제 벡터 (discourse vector)라는 개념을 도입했다. 주제 벡터 c 가 주어졌을 때 어떤 단어 w가 나타날 확률을 정의했다.

    우선 첫 항은 단어 w가 주제와 상관없이 등장할 확률을 나타냈다. 한국어를 예로 들면 조사 같은 경우 첫 상이 높은 값을 가질 것이다.

    그 다음 항은 단어 w가 주제와 관련을 가질 확률을 의미한다. 주제 벡터 c와 w에 해당하는 단어 벡터 v가 유사할수록 (내적 값이 클수록) 그 값이 커진다.

    이 두 항을 가중합을 취해 최종적인 등장 확률을 계산한다. 첫 항에 변수 알파, 두 번째 항은 1-알파를 곱해주는데, 알파는 사용자가 지정하는 하이퍼파라미터다.

    우리가 관찰하고 있는 단어 w가 등장할 확률을 최대화하는 주제 벡터 c를 찾는 것이 목표다. w가 등장할 확률을 최대화하는 주제 벡터를 찾게 된다면 이 주제 벡터는 해당 단어의 사용을 제일 잘 설명하는 주제 벡터가 될 것이다.

    우리가 관찰하고 있는 문장이 등장할 확률을 최대화하는 주제 벡터는 문장에 속한 단어들에 해당하는 단어 벡터에 가중치를 곱해 마든 새로운 벡터들의 합에 비례한다. 새로운 단어 벡터를 만들 때의 가중치는 해당 단어가 말뭉치에 얼마나 자주 등장하는지를 감안해 만든다. 

2. 모델 구현

    책에 있는 코드랑 나중에 참고해서 직접 해볼 때 뒤로 더 작성하는게 나을 듯