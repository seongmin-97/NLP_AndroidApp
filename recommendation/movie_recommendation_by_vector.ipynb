{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class movie_recommendation_vec:\n",
    "    def __init__(self, **kargs):\n",
    "        self.topn = kargs.get('topn',10)\n",
    "        self.vec_type = kargs.get('vec_type','tfidf')\n",
    "        self.df = kargs.get('data', pd.read_csv('./data/merged.csv'))\n",
    "        self.a, self.b, self.c = kargs.get('a', 0.6), kargs.get('b', 0.2), kargs.get('c', 0.2)\n",
    "        self.vote_thres = kargs.get('vote_thres', 100)\n",
    "        self.verbose = kargs.get('verbose', 1)\n",
    "        \n",
    "        self.cvec = CountVectorizer(min_df=0, ngram_range=(1,2))\n",
    "        self.stops = []\n",
    "        with open('./data/total_stopwords', encoding='utf-8') as f:\n",
    "            self.stops.append(f.readline()[:-2])\n",
    "        \n",
    "        if self.vec_type == 'tfidf':\n",
    "            self.vec = TfidfVectorizer(analyzer='word', ngram_range=(1,5),stop_words=self.stops,\n",
    "                       min_df=10, max_df=0.95, max_features=10000)\n",
    "        elif self.vec_type == 'count':\n",
    "            self.vec = CountVectorizer(analyzer='word',ngram_range=(1,1), stop_words=self.stops,\n",
    "                      min_df=10, max_df=0.95, max_features=10000)\n",
    "        else:\n",
    "            raise ValueError('vec_type is not in [tfidf, count]')\n",
    "            \n",
    "        if self.verbose == 1:\n",
    "            print('-'*35)\n",
    "            print('# Parameters')\n",
    "            print('      a, b, c        : {0}, {1}, {2}'.format(self.a, self.b, self.c))\n",
    "            print('vote count threshold :', self.vote_thres)\n",
    "            print('vec_type :',self.vec_type.capitalize())\n",
    "            print('weighted_sum = vec_sim_scaled*{0}(a) + genre_scaled*{1}(b) + wvote_scaled*{2}(c)'.format(self.a, self.b, self.c))\n",
    "            print('-'*35)\n",
    "            \n",
    "    def search_title(self, title_name):\n",
    "        return self.df[self.df['title'].str.contains(title_name)].title\n",
    "    \n",
    "    def genre_sim_sorted(self, title_idx):\n",
    "        genre_literal = self.df['genre'].apply(lambda x: x.replace('|',' '))\n",
    "        genre = self.cvec.fit_transform(genre_literal)\n",
    "        genre_sim = cosine_similarity(genre,genre)\n",
    "\n",
    "        return np.array([(idx,sim) for idx,sim in enumerate(genre_sim[title_idx])])            \n",
    "\n",
    "    def raw_to_vec(self, data_preprocess):\n",
    "        return self.vec.fit_transform(data_preprocess)\n",
    "    \n",
    "    def cos_sim(self, data_preprocess, title_idx):\n",
    "        cos_sims = []\n",
    "        data_transformed = self.raw_to_vec(data_preprocess)\n",
    "        \n",
    "        for df_idx, src_idx in zip(self.df.index, range(data_transformed.shape[0])):\n",
    "            if df_idx != title_idx:\n",
    "                cos_sims.append((df_idx, cosine_similarity(data_transformed[title_idx].reshape(1,-1),\n",
    "                                                          data_transformed[src_idx].reshape(1,-1))))\n",
    "        return cos_sims\n",
    "    \n",
    "    def similar_vec_movies(self, title_idx):\n",
    "        idx_sims = np.array(self.cos_sim(self.df['plot_preprocessed_kkma'], title_idx))\n",
    "        sims_scaled = MinMaxScaler().fit_transform(idx_sims[:,1].reshape(-1,1))\n",
    "        idx_sims[:,1] = sims_scaled.reshape(-1)\n",
    "\n",
    "        idx_sims = np.array(sorted(idx_sims, key=lambda x: x[1], reverse=True))\n",
    "        result_df = self.df.loc[idx_sims[:,0]]\n",
    "        result_df['vec_sim'] = idx_sims[:,1]\n",
    "        \n",
    "        return result_df[result_df['vote_count'] > self.vote_thres]\n",
    "    \n",
    "    def result_by_weights(self, dataf):\n",
    "        dataf['weighted_sum'] = dataf['vec_sim_scaled']*self.a + dataf['genre_scaled']*self.b + dataf['wvote_scaled']*self.c\n",
    "        \n",
    "        return dataf.sort_values('weighted_sum', ascending=False)\n",
    "    \n",
    "    def result_by_weights_negative(self, dataf):\n",
    "        dataf['weighted_sum'] = dataf['vec_sim_scaled']*self.a + dataf['genre_scaled']*self.b + dataf['wvote_scaled']*self.c\n",
    "        \n",
    "        return dataf.sort_values('weighted_sum', ascending=True)\n",
    "    \n",
    "    def getMovies_negative(self, title):\n",
    "        # no title result\n",
    "        try:title_idx = self.df[self.df['title']== title].index.values[0]\n",
    "        except:\n",
    "            raise ValueError('There is no such title name in data. Search with \"search_title\" function')\n",
    "\n",
    "        # get movies\n",
    "        result = self.similar_vec_movies(title_idx)\n",
    "        \n",
    "        # IMDB's weighted_vote\n",
    "        def weighted_vote_average(record):\n",
    "            v, r = record['vote_count'], record['rating']\n",
    "            return (v/(v+m))*r + (m/(m+v))*c\n",
    "        c = result['rating'].mean()\n",
    "        m = result['vote_count'].quantile(.6)\n",
    "        result['weighted_vote'] = result.apply(weighted_vote_average,axis=1)\n",
    "        \n",
    "        # merge with genre\n",
    "        genre_sim = self.genre_sim_sorted(title_idx)\n",
    "        Result = result.rename(columns={'key_0': 'key_00'})\n",
    "        result_with_genre = pd.merge(Result, pd.Series(genre_sim[:,1], name='genre_sim'), left_on=Result.index, right_on=genre_sim[:,0],)\n",
    "        \n",
    "        # minmax scale\n",
    "        result_with_genre['vec_sim_scaled'] = MinMaxScaler().fit_transform(result_with_genre['vec_sim'].values.reshape(-1,1))\n",
    "        result_with_genre['wvote_scaled'] = MinMaxScaler().fit_transform(result_with_genre['weighted_vote'].values.reshape(-1,1))\n",
    "        result_with_genre['genre_scaled'] = MinMaxScaler().fit_transform(result_with_genre['genre_sim'].values.reshape(-1,1))\n",
    "\n",
    "        # (optional)remove data genre score is 0\n",
    "        no_genre_score_idx = result_with_genre[result_with_genre['genre_sim'] == 0].index\n",
    "        result_with_genre.drop(no_genre_score_idx, inplace=True)\n",
    "        \n",
    "        result_with_genre = self.result_by_weights_negative(result_with_genre)\n",
    "        return result_with_genre.head(self.topn)\n",
    "    \n",
    "    def getMovies(self, title):\n",
    "        # no title result\n",
    "        try:title_idx = self.df[self.df['title']== title].index.values[0]\n",
    "        except:\n",
    "            raise ValueError('There is no such title name in data. Search with \"search_title\" function')\n",
    "\n",
    "        # get movies\n",
    "        result = self.similar_vec_movies(title_idx)\n",
    "        \n",
    "        # IMDB's weighted_vote\n",
    "        def weighted_vote_average(record):\n",
    "            v, r = record['vote_count'], record['rating']\n",
    "            return (v/(v+m))*r + (m/(m+v))*c\n",
    "        c = result['rating'].mean()\n",
    "        m = result['vote_count'].quantile(.6)\n",
    "        result['weighted_vote'] = result.apply(weighted_vote_average,axis=1)\n",
    "        \n",
    "        # merge with genre\n",
    "        genre_sim = self.genre_sim_sorted(title_idx)\n",
    "        Result = result.rename(columns={'key_0': 'key_00'})\n",
    "        result_with_genre = pd.merge(Result, pd.Series(genre_sim[:,1], name='genre_sim'), left_on=Result.index, right_on=genre_sim[:,0],)\n",
    "        \n",
    "        # minmax scale\n",
    "        result_with_genre['vec_sim_scaled'] = MinMaxScaler().fit_transform(result_with_genre['vec_sim'].values.reshape(-1,1))\n",
    "        result_with_genre['wvote_scaled'] = MinMaxScaler().fit_transform(result_with_genre['weighted_vote'].values.reshape(-1,1))\n",
    "        result_with_genre['genre_scaled'] = MinMaxScaler().fit_transform(result_with_genre['genre_sim'].values.reshape(-1,1))\n",
    "\n",
    "        # (optional)remove data genre score is 0\n",
    "        no_genre_score_idx = result_with_genre[result_with_genre['genre_sim'] == 0].index\n",
    "        result_with_genre.drop(no_genre_score_idx, inplace=True)\n",
    "        \n",
    "        result_with_genre = self.result_by_weights(result_with_genre)\n",
    "        return result_with_genre.head(self.topn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "# Parameters\n",
      "      a, b, c        : 0.6, 0.2, 0.2\n",
      "vote count threshold : 100\n",
      "vec_type : Tfidf\n",
      "weighted_sum = vec_sim_scaled*0.6(a) + genre_scaled*0.2(b) + wvote_scaled*0.2(c)\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "recom = movie_recommendation_vec(vec_type='tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = recom.getMovies(title='아이언맨 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                     아이언맨 3\n",
       "1                       아이언맨\n",
       "2                        로보캅\n",
       "4                       어벤져스\n",
       "3                        앤트맨\n",
       "5                 스파이더맨: 홈커밍\n",
       "203             스타 트렉: 더 비기닝\n",
       "457     스타워즈 에피소드 3 - 시스의 복수\n",
       "1598                    아일랜드\n",
       "187            매트릭스 2 - 리로디드\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = recom.getMovies_negative(title='아이언맨 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_0</th>\n",
       "      <th>key_00</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>genre</th>\n",
       "      <th>year</th>\n",
       "      <th>date</th>\n",
       "      <th>rating</th>\n",
       "      <th>vote_count</th>\n",
       "      <th>plot</th>\n",
       "      <th>...</th>\n",
       "      <th>img_url</th>\n",
       "      <th>keywords</th>\n",
       "      <th>plot_preprocessed_kkma</th>\n",
       "      <th>vec_sim</th>\n",
       "      <th>weighted_vote</th>\n",
       "      <th>genre_sim</th>\n",
       "      <th>vec_sim_scaled</th>\n",
       "      <th>wvote_scaled</th>\n",
       "      <th>genre_scaled</th>\n",
       "      <th>weighted_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4334</th>\n",
       "      <td>6959</td>\n",
       "      <td>6959</td>\n",
       "      <td>6960</td>\n",
       "      <td>쾌걸 조로리의 대대대대모험</td>\n",
       "      <td>애니메이션|모험</td>\n",
       "      <td>2013</td>\n",
       "      <td>6.05</td>\n",
       "      <td>1.27</td>\n",
       "      <td>2599</td>\n",
       "      <td>아주 오랜 옛날, 해적들이 수많은 보물을 숨겨 둔 전설의 가파르산! 가파르산의 작은...</td>\n",
       "      <td>...</td>\n",
       "      <td>https://movie-phinf.pstatic.net/20130513_201/1...</td>\n",
       "      <td>['해적', '모아', '사이에서', '수수께끼', '전설', '마을', '힘', ...</td>\n",
       "      <td>옛날 해적 보물 숨기 전설 가 파 르 산 마을 가 파파 조용하 덜 마을 대소동 일어...</td>\n",
       "      <td>0.0246006</td>\n",
       "      <td>2.762994</td>\n",
       "      <td>0.258199</td>\n",
       "      <td>0.024601</td>\n",
       "      <td>0.051747</td>\n",
       "      <td>0.258199</td>\n",
       "      <td>0.076749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5050</th>\n",
       "      <td>10596</td>\n",
       "      <td>10596</td>\n",
       "      <td>10599</td>\n",
       "      <td>썬데이 서울</td>\n",
       "      <td>코미디|SF|판타지</td>\n",
       "      <td>2006</td>\n",
       "      <td>2.09</td>\n",
       "      <td>3.15</td>\n",
       "      <td>1865</td>\n",
       "      <td>생긴 것도 억울한데 왕따까지 당하는 반친구 도연(봉태규 분)에게 일어난 엄청난 신체...</td>\n",
       "      <td>...</td>\n",
       "      <td>https://movie-phinf.pstatic.net/20111223_167/1...</td>\n",
       "      <td>['연', '배달', '이야기', '무술', '무협', '에피소드', '짜장면', ...</td>\n",
       "      <td>생기 왕따 당하 일어나 신체적 변화 짜장면 배달 가 사건 현장 살해 일어나 사건 고...</td>\n",
       "      <td>0.0186208</td>\n",
       "      <td>4.490493</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.018621</td>\n",
       "      <td>0.298706</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.110914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5783</th>\n",
       "      <td>9057</td>\n",
       "      <td>9057</td>\n",
       "      <td>9059</td>\n",
       "      <td>나는 비와 함께 간다</td>\n",
       "      <td>스릴러|범죄|액션</td>\n",
       "      <td>2009</td>\n",
       "      <td>10.15</td>\n",
       "      <td>3.77</td>\n",
       "      <td>2438</td>\n",
       "      <td>전직 형사 클라인(조쉬 하트넷)은 어느 날 대부호로부터 실종된 아들을 찾아달라는 의...</td>\n",
       "      <td>...</td>\n",
       "      <td>https://movie-phinf.pstatic.net/20111223_83/13...</td>\n",
       "      <td>['실종', '타오', '마피아', '쿠', '대부호', '릴리', '홍콩', '남...</td>\n",
       "      <td>전직 날 대부 호로 실종 아들 찾 닿 의뢰 받 이름 시 타 오 기무 타 야 시 타 ...</td>\n",
       "      <td>0.0121572</td>\n",
       "      <td>4.720168</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.012157</td>\n",
       "      <td>0.331540</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.113602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4987</th>\n",
       "      <td>10110</td>\n",
       "      <td>10110</td>\n",
       "      <td>10113</td>\n",
       "      <td>상사부일체 - 두사부일체 3</td>\n",
       "      <td>코미디|액션</td>\n",
       "      <td>2007</td>\n",
       "      <td>9.19</td>\n",
       "      <td>3.25</td>\n",
       "      <td>2780</td>\n",
       "      <td>드디어 대학교 졸업장을 따고 강남을 맡게 된 계두식. 조직의 구조를 글로벌 하게 만...</td>\n",
       "      <td>...</td>\n",
       "      <td>https://movie-phinf.pstatic.net/20111223_112/1...</td>\n",
       "      <td>['조직', '회사', '실', '조직원', '보험', '글로벌', '기대', '기...</td>\n",
       "      <td>대학교 졸업장 따 강남 맡 되 계두 식 조직 구조 글로벌 하 만들 큰형 하명 따르 ...</td>\n",
       "      <td>0.019061</td>\n",
       "      <td>4.225461</td>\n",
       "      <td>0.258199</td>\n",
       "      <td>0.019061</td>\n",
       "      <td>0.260817</td>\n",
       "      <td>0.258199</td>\n",
       "      <td>0.115240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4089</th>\n",
       "      <td>5871</td>\n",
       "      <td>5871</td>\n",
       "      <td>5872</td>\n",
       "      <td>조선미녀삼총사</td>\n",
       "      <td>코미디|액션|드라마</td>\n",
       "      <td>2014</td>\n",
       "      <td>1.29</td>\n",
       "      <td>4.04</td>\n",
       "      <td>4907</td>\n",
       "      <td>죄명 불문! 상대 불문! 완벽한 검거율을 자랑하는 조선 팔도 최고의 현상금 사냥꾼이...</td>\n",
       "      <td>...</td>\n",
       "      <td>https://movie-phinf.pstatic.net/20140108_218/1...</td>\n",
       "      <td>['자랑', '조선', '주먹', '시크', '한', '리더', '실력파', '팔도...</td>\n",
       "      <td>죄명 불문 상대 불문 완벽 검거 자랑 조선 팔도 최고 현상금 사냥꾼 나타나 으뜸가 ...</td>\n",
       "      <td>0.0264315</td>\n",
       "      <td>4.541221</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.026431</td>\n",
       "      <td>0.305958</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.117050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6599</th>\n",
       "      <td>11573</td>\n",
       "      <td>11573</td>\n",
       "      <td>11577</td>\n",
       "      <td>포가튼</td>\n",
       "      <td>드라마|미스터리|SF|스릴러</td>\n",
       "      <td>2004</td>\n",
       "      <td>12.03</td>\n",
       "      <td>4.47</td>\n",
       "      <td>965</td>\n",
       "      <td>비행기 사고로 아들을 잃은 불행한 기억으로 괴로워 하던 텔리(줄리언 무어)는 정신과...</td>\n",
       "      <td>...</td>\n",
       "      <td>https://movie-phinf.pstatic.net/20111222_25/13...</td>\n",
       "      <td>['아들', '기억', '정신', '남편', '샘', '상상', '속', '존재',...</td>\n",
       "      <td>비행기 사고 아들 잃 불행 기억 정신과 상담 치료 시작하 슬픔 지우 위하 행복 기억...</td>\n",
       "      <td>0.00134798</td>\n",
       "      <td>5.896527</td>\n",
       "      <td>0.169031</td>\n",
       "      <td>0.001348</td>\n",
       "      <td>0.499709</td>\n",
       "      <td>0.169031</td>\n",
       "      <td>0.134557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6376</th>\n",
       "      <td>10377</td>\n",
       "      <td>10377</td>\n",
       "      <td>10380</td>\n",
       "      <td>게드전기 - 어스시의 전설</td>\n",
       "      <td>애니메이션|판타지|가족|모험</td>\n",
       "      <td>2006</td>\n",
       "      <td>8.10</td>\n",
       "      <td>4.99</td>\n",
       "      <td>1705</td>\n",
       "      <td>용이 출몰하고 마법이 존재하는 ‘어스시(E rthse )’의 세계에서 펼쳐지는 마법...</td>\n",
       "      <td>...</td>\n",
       "      <td>https://movie-phinf.pstatic.net/20111221_227/1...</td>\n",
       "      <td>['누', '바닷가', '테', '하', '출몰', '마법', '존재', '미국인'...</td>\n",
       "      <td>용 출몰 마법 존재 어스 시 의 세계 펼쳐지 마법사 왕자 모험 이야기 미국인 여성 ...</td>\n",
       "      <td>0.00512937</td>\n",
       "      <td>5.829023</td>\n",
       "      <td>0.169031</td>\n",
       "      <td>0.005129</td>\n",
       "      <td>0.490059</td>\n",
       "      <td>0.169031</td>\n",
       "      <td>0.134896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4328</th>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>국제수사</td>\n",
       "      <td>액션|드라마|범죄</td>\n",
       "      <td>2020</td>\n",
       "      <td>9.29</td>\n",
       "      <td>4.64</td>\n",
       "      <td>3312</td>\n",
       "      <td>필리핀으로 인생 첫 해외여행을 떠난 대천경찰서 강력팀 ‘홍병수’(곽도원) 경장. 여...</td>\n",
       "      <td>...</td>\n",
       "      <td>https://movie-phinf.pstatic.net/20200916_163/1...</td>\n",
       "      <td>['수사', '범죄', '형사', '용의자', '의', '필리핀', '꿈', '마음...</td>\n",
       "      <td>필리핀 인생 해외여행 떠나 대천 경찰서 강력 팀 경장 여행 단꿈 잠시 범죄 조직 킬...</td>\n",
       "      <td>0.0246543</td>\n",
       "      <td>5.219072</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.024654</td>\n",
       "      <td>0.402862</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.135365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6606</th>\n",
       "      <td>7773</td>\n",
       "      <td>7773</td>\n",
       "      <td>7774</td>\n",
       "      <td>타이타닉 2</td>\n",
       "      <td>액션|모험</td>\n",
       "      <td>2012</td>\n",
       "      <td>4.25</td>\n",
       "      <td>2.23</td>\n",
       "      <td>2384</td>\n",
       "      <td>타이타닉호 침몰이 있은 후 백 년 뒤, 예전의 타이타닉호 보다 한층 고급스럽고 거대...</td>\n",
       "      <td>...</td>\n",
       "      <td>https://movie-phinf.pstatic.net/20120412_116/1...</td>\n",
       "      <td>['항해', '엔진', '안전', '대서양', '횡단', '축하', '진행', '기...</td>\n",
       "      <td>타이 타 호 침몰 있 후 뒤 예전 타이 타 호 고급 거대 최첨단 선박 타이 타 이름...</td>\n",
       "      <td>0.00103771</td>\n",
       "      <td>3.582860</td>\n",
       "      <td>0.516398</td>\n",
       "      <td>0.001038</td>\n",
       "      <td>0.168953</td>\n",
       "      <td>0.516398</td>\n",
       "      <td>0.137693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5081</th>\n",
       "      <td>4196</td>\n",
       "      <td>4196</td>\n",
       "      <td>4197</td>\n",
       "      <td>나의 절친 악당들</td>\n",
       "      <td>액션|범죄|드라마</td>\n",
       "      <td>2015</td>\n",
       "      <td>6.25</td>\n",
       "      <td>4.81</td>\n",
       "      <td>2793</td>\n",
       "      <td>인턴 지누에게 첫 번째 임무가 내려진다. 그동안 감시해온 차량의 이동라인을 완벽하게...</td>\n",
       "      <td>...</td>\n",
       "      <td>https://movie-phinf.pstatic.net/20150513_61/14...</td>\n",
       "      <td>['차량', '파', '표적', '위험천만', '라인', '완벽', '충돌', '악...</td>\n",
       "      <td>일 턴 누 임무 내려지 그동안 감시 오 차량 이동 라인 완벽 파악 보 하 뒤 쫓 차...</td>\n",
       "      <td>0.0183793</td>\n",
       "      <td>5.434926</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.018379</td>\n",
       "      <td>0.433720</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.137772</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      key_0  key_00  Unnamed: 0            title            genre  year  \\\n",
       "4334   6959    6959        6960   쾌걸 조로리의 대대대대모험         애니메이션|모험  2013   \n",
       "5050  10596   10596       10599           썬데이 서울       코미디|SF|판타지  2006   \n",
       "5783   9057    9057        9059      나는 비와 함께 간다        스릴러|범죄|액션  2009   \n",
       "4987  10110   10110       10113  상사부일체 - 두사부일체 3           코미디|액션  2007   \n",
       "4089   5871    5871        5872          조선미녀삼총사       코미디|액션|드라마  2014   \n",
       "6599  11573   11573       11577              포가튼  드라마|미스터리|SF|스릴러  2004   \n",
       "6376  10377   10377       10380   게드전기 - 어스시의 전설  애니메이션|판타지|가족|모험  2006   \n",
       "4328     48      48          48             국제수사        액션|드라마|범죄  2020   \n",
       "6606   7773    7773        7774           타이타닉 2            액션|모험  2012   \n",
       "5081   4196    4196        4197        나의 절친 악당들        액션|범죄|드라마  2015   \n",
       "\n",
       "       date  rating  vote_count  \\\n",
       "4334   6.05    1.27        2599   \n",
       "5050   2.09    3.15        1865   \n",
       "5783  10.15    3.77        2438   \n",
       "4987   9.19    3.25        2780   \n",
       "4089   1.29    4.04        4907   \n",
       "6599  12.03    4.47         965   \n",
       "6376   8.10    4.99        1705   \n",
       "4328   9.29    4.64        3312   \n",
       "6606   4.25    2.23        2384   \n",
       "5081   6.25    4.81        2793   \n",
       "\n",
       "                                                   plot  ...  \\\n",
       "4334  아주 오랜 옛날, 해적들이 수많은 보물을 숨겨 둔 전설의 가파르산! 가파르산의 작은...  ...   \n",
       "5050  생긴 것도 억울한데 왕따까지 당하는 반친구 도연(봉태규 분)에게 일어난 엄청난 신체...  ...   \n",
       "5783  전직 형사 클라인(조쉬 하트넷)은 어느 날 대부호로부터 실종된 아들을 찾아달라는 의...  ...   \n",
       "4987  드디어 대학교 졸업장을 따고 강남을 맡게 된 계두식. 조직의 구조를 글로벌 하게 만...  ...   \n",
       "4089  죄명 불문! 상대 불문! 완벽한 검거율을 자랑하는 조선 팔도 최고의 현상금 사냥꾼이...  ...   \n",
       "6599  비행기 사고로 아들을 잃은 불행한 기억으로 괴로워 하던 텔리(줄리언 무어)는 정신과...  ...   \n",
       "6376  용이 출몰하고 마법이 존재하는 ‘어스시(E rthse )’의 세계에서 펼쳐지는 마법...  ...   \n",
       "4328  필리핀으로 인생 첫 해외여행을 떠난 대천경찰서 강력팀 ‘홍병수’(곽도원) 경장. 여...  ...   \n",
       "6606  타이타닉호 침몰이 있은 후 백 년 뒤, 예전의 타이타닉호 보다 한층 고급스럽고 거대...  ...   \n",
       "5081  인턴 지누에게 첫 번째 임무가 내려진다. 그동안 감시해온 차량의 이동라인을 완벽하게...  ...   \n",
       "\n",
       "                                                img_url  \\\n",
       "4334  https://movie-phinf.pstatic.net/20130513_201/1...   \n",
       "5050  https://movie-phinf.pstatic.net/20111223_167/1...   \n",
       "5783  https://movie-phinf.pstatic.net/20111223_83/13...   \n",
       "4987  https://movie-phinf.pstatic.net/20111223_112/1...   \n",
       "4089  https://movie-phinf.pstatic.net/20140108_218/1...   \n",
       "6599  https://movie-phinf.pstatic.net/20111222_25/13...   \n",
       "6376  https://movie-phinf.pstatic.net/20111221_227/1...   \n",
       "4328  https://movie-phinf.pstatic.net/20200916_163/1...   \n",
       "6606  https://movie-phinf.pstatic.net/20120412_116/1...   \n",
       "5081  https://movie-phinf.pstatic.net/20150513_61/14...   \n",
       "\n",
       "                                               keywords  \\\n",
       "4334  ['해적', '모아', '사이에서', '수수께끼', '전설', '마을', '힘', ...   \n",
       "5050  ['연', '배달', '이야기', '무술', '무협', '에피소드', '짜장면', ...   \n",
       "5783  ['실종', '타오', '마피아', '쿠', '대부호', '릴리', '홍콩', '남...   \n",
       "4987  ['조직', '회사', '실', '조직원', '보험', '글로벌', '기대', '기...   \n",
       "4089  ['자랑', '조선', '주먹', '시크', '한', '리더', '실력파', '팔도...   \n",
       "6599  ['아들', '기억', '정신', '남편', '샘', '상상', '속', '존재',...   \n",
       "6376  ['누', '바닷가', '테', '하', '출몰', '마법', '존재', '미국인'...   \n",
       "4328  ['수사', '범죄', '형사', '용의자', '의', '필리핀', '꿈', '마음...   \n",
       "6606  ['항해', '엔진', '안전', '대서양', '횡단', '축하', '진행', '기...   \n",
       "5081  ['차량', '파', '표적', '위험천만', '라인', '완벽', '충돌', '악...   \n",
       "\n",
       "                                 plot_preprocessed_kkma     vec_sim  \\\n",
       "4334  옛날 해적 보물 숨기 전설 가 파 르 산 마을 가 파파 조용하 덜 마을 대소동 일어...   0.0246006   \n",
       "5050  생기 왕따 당하 일어나 신체적 변화 짜장면 배달 가 사건 현장 살해 일어나 사건 고...   0.0186208   \n",
       "5783  전직 날 대부 호로 실종 아들 찾 닿 의뢰 받 이름 시 타 오 기무 타 야 시 타 ...   0.0121572   \n",
       "4987  대학교 졸업장 따 강남 맡 되 계두 식 조직 구조 글로벌 하 만들 큰형 하명 따르 ...    0.019061   \n",
       "4089  죄명 불문 상대 불문 완벽 검거 자랑 조선 팔도 최고 현상금 사냥꾼 나타나 으뜸가 ...   0.0264315   \n",
       "6599  비행기 사고 아들 잃 불행 기억 정신과 상담 치료 시작하 슬픔 지우 위하 행복 기억...  0.00134798   \n",
       "6376  용 출몰 마법 존재 어스 시 의 세계 펼쳐지 마법사 왕자 모험 이야기 미국인 여성 ...  0.00512937   \n",
       "4328  필리핀 인생 해외여행 떠나 대천 경찰서 강력 팀 경장 여행 단꿈 잠시 범죄 조직 킬...   0.0246543   \n",
       "6606  타이 타 호 침몰 있 후 뒤 예전 타이 타 호 고급 거대 최첨단 선박 타이 타 이름...  0.00103771   \n",
       "5081  일 턴 누 임무 내려지 그동안 감시 오 차량 이동 라인 완벽 파악 보 하 뒤 쫓 차...   0.0183793   \n",
       "\n",
       "     weighted_vote genre_sim vec_sim_scaled  wvote_scaled  genre_scaled  \\\n",
       "4334      2.762994  0.258199       0.024601      0.051747      0.258199   \n",
       "5050      4.490493  0.200000       0.018621      0.298706      0.200000   \n",
       "5783      4.720168  0.200000       0.012157      0.331540      0.200000   \n",
       "4987      4.225461  0.258199       0.019061      0.260817      0.258199   \n",
       "4089      4.541221  0.200000       0.026431      0.305958      0.200000   \n",
       "6599      5.896527  0.169031       0.001348      0.499709      0.169031   \n",
       "6376      5.829023  0.169031       0.005129      0.490059      0.169031   \n",
       "4328      5.219072  0.200000       0.024654      0.402862      0.200000   \n",
       "6606      3.582860  0.516398       0.001038      0.168953      0.516398   \n",
       "5081      5.434926  0.200000       0.018379      0.433720      0.200000   \n",
       "\n",
       "      weighted_sum  \n",
       "4334      0.076749  \n",
       "5050      0.110914  \n",
       "5783      0.113602  \n",
       "4987      0.115240  \n",
       "4089      0.117050  \n",
       "6599      0.134557  \n",
       "6376      0.134896  \n",
       "4328      0.135365  \n",
       "6606      0.137693  \n",
       "5081      0.137772  \n",
       "\n",
       "[10 rows x 23 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        베스트 오브 미\n",
       "1         고양이 장례식\n",
       "6       그 남자의 사랑법\n",
       "3         사랑해, 파리\n",
       "2            아일랜드\n",
       "9       번지 점프를 하다\n",
       "4     날 미치게 하는 남자\n",
       "18          오 루시!\n",
       "14          스타더스트\n",
       "58           노팅 힐\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = recom.getMovies(\"어바웃 타임\")\n",
    "result['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "베스트 오브 미\n",
      "고양이 장례식\n",
      "그 남자의 사랑법\n",
      "사랑해, 파리\n",
      "아일랜드\n",
      "번지 점프를 하다\n",
      "날 미치게 하는 남자\n",
      "오 루시!\n",
      "스타더스트\n",
      "노팅 힐\n"
     ]
    }
   ],
   "source": [
    "for i in result['title'] :\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "# Parameters\n",
      "      a, b, c        : 0.6, 0.2, 0.2\n",
      "vote count threshold : 100\n",
      "vec_type : Tfidf\n",
      "weighted_sum = vec_sim_scaled*0.6(a) + genre_scaled*0.2(b) + wvote_scaled*0.2(c)\n",
      "-----------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf2f405bc66249dab4cdef3d80603278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-b85745cadf31>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'1st_movie'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'2nd_movie'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'3rd_movie'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'4th_movie'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'5th_movie'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'6th_movie'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'7th_movie'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'8th_movie'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'9th_movie'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'10th_movie'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmovie_idx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetMovies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmovie\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmovie_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-a601adc42562>\u001b[0m in \u001b[0;36mgetMovies\u001b[1;34m(self, title)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;31m# get movies\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimilar_vec_movies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;31m# IMDB's weighted_vote\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-a601adc42562>\u001b[0m in \u001b[0;36msimilar_vec_movies\u001b[1;34m(self, title_idx)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msimilar_vec_movies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0midx_sims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcos_sim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'plot_preprocessed_kkma'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         \u001b[0msims_scaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx_sims\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0midx_sims\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msims_scaled\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-a601adc42562>\u001b[0m in \u001b[0;36mcos_sim\u001b[1;34m(self, data_preprocess, title_idx)\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdf_idx\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mtitle_idx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m                 cos_sims.append((df_idx, cosine_similarity(data_transformed[title_idx].reshape(1,-1),\n\u001b[1;32m---> 53\u001b[1;33m                                                           data_transformed[src_idx].reshape(1,-1))))\n\u001b[0m\u001b[0;32m     54\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcos_sims\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m     K = safe_sparse_dot(X_normalized, Y_normalized.T,\n\u001b[1;32m-> 1187\u001b[1;33m                         dense_output=dense_output)\n\u001b[0m\u001b[0;32m   1188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1189\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__matmul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    558\u001b[0m             raise ValueError(\"Scalar operands are not allowed, \"\n\u001b[0;32m    559\u001b[0m                              \"use '*' instead\")\n\u001b[1;32m--> 560\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__mul__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__rmatmul__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__mul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    478\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dimension mismatch'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 480\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mul_sparse_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    481\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[1;31m# If it's a list or whatever, treat it like a matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36m_mul_sparse_matrix\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[0mmajor_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m         \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# convert to this format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m         idx_dtype = get_index_dtype((self.indptr, self.indices,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[0;32m     32\u001b[0m                 \u001b[0marg1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m                 \u001b[0marg1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_self\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36masformat\u001b[1;34m(self, format, copy)\u001b[0m\n\u001b[0;32m    320\u001b[0m             \u001b[1;31m# Forward the copy kwarg, if it's accepted.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mconvert_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mconvert_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\csc.py\u001b[0m in \u001b[0;36mtocsr\u001b[1;34m(self, copy)\u001b[0m\n\u001b[0;32m    146\u001b[0m                   \u001b[0mindptr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m                   \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m                   data)\n\u001b[0m\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcsr\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "movie = pd.read_csv('./data/merged.csv')\n",
    "recom = movie_recommendation_vec(vec_type='tfidf')\n",
    "columns = ['1st_movie', '2nd_movie', '3rd_movie', '4th_movie', '5th_movie', '6th_movie', '7th_movie', '8th_movie', '9th_movie', '10th_movie']\n",
    "error = 0\n",
    "for movie_idx in tqdm(range(movie['title'])) :\n",
    "    try :\n",
    "        results = recom.getMovies(movie['title'][movie_idx])\n",
    "        data = {}\n",
    "        index = 0\n",
    "        for result in results['title'] :\n",
    "            data[\"title\"] = movie['title'][movie_idx]\n",
    "            data[columns[index]] = result\n",
    "            index += 1\n",
    "        with open(\"./data/recommend_movie.csv\", \"a\", encoding=\"utf-8\", newline=\"\") as csvfile :\n",
    "            fieldnames = ['title'] + columns\n",
    "            csvwriter = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            csvwriter.writerow(data)\n",
    "    except :\n",
    "        error += 1\n",
    "        print(\"error is \"+ error)\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
